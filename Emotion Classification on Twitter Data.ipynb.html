<h1 id="classification">Classification</h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="op">!</span>pip install gensim</span></code></pre></div>
<pre><code>Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)
Requirement already satisfied: numpy&gt;=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)
Requirement already satisfied: scipy&gt;=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)
Requirement already satisfied: smart_open&gt;=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)
Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open&gt;=1.8.1-&gt;gensim) (2.0.1)</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a><span class="im">import</span> re</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, classification_report</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;/content/Twitter_Emotion_Dataset.csv&quot;</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>df.head</span></code></pre></div>
<div style="max-width:800px; border: 1px solid var(--colab-border-color);">
<style>
      pre.function-repr-contents {
        overflow-x: auto;
        padding: 8px 12px;
        max-height: 500px;
      }

      pre.function-repr-contents.function-repr-contents-collapsed {
        cursor: pointer;
        max-height: 100px;
      }
    </style>
<pre><code>&lt;pre style=&quot;white-space: initial; background:
     var(--colab-secondary-surface-color); padding: 8px 12px;
     border-bottom: 1px solid var(--colab-border-color);&quot;&gt;&lt;b&gt;pandas.core.generic.NDFrame.head&lt;/b&gt;&lt;br/&gt;def head(n: int=5) -&amp;gt; Self&lt;/pre&gt;&lt;pre class=&quot;function-repr-contents function-repr-contents-collapsed&quot; style=&quot;&quot;&gt;&lt;a class=&quot;filepath&quot; style=&quot;display:none&quot; href=&quot;#&quot;&gt;/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py&lt;/a&gt;Return the first `n` rows.</code></pre>
<p>This function returns the first <code>n</code> rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it.</p>
<p>For negative values of <code>n</code>, this function returns all rows except the last <code>|n|</code> rows, equivalent to <code>df[:n]</code>.</p>
<p>If n is larger than the number of rows, this function returns all rows.</p>
<h2 id="parameters">Parameters</h2>
<p>n : int, default 5 Number of rows to select.</p>
<h2 id="returns">Returns</h2>
<p>same type as caller The first <code>n</code> rows of the caller object.</p>
<h2 id="see-also">See Also</h2>
<p>DataFrame.tail: Returns the last <code>n</code> rows.</p>
<h2 id="examples">Examples</h2>
<p>&gt;&gt;&gt; df = pd.DataFrame({‘animal’: [‘alligator’, ‘bee’, ‘falcon’, ‘lion’, … ‘monkey’, ‘parrot’, ‘shark’, ‘whale’, ‘zebra’]}) &gt;&gt;&gt; df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra</p>
<p>Viewing the first 5 lines</p>
<p>&gt;&gt;&gt; df.head() animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey</p>
<p>Viewing the first <code>n</code> lines (three in this case)</p>
<p>&gt;&gt;&gt; df.head(3) animal 0 alligator 1 bee 2 falcon</p>
<p>For negative values of <code>n</code></p>
&gt;&gt;&gt; df.head(-3) animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot
</pre>
<pre><code>  &lt;script&gt;
  if (google.colab.kernel.accessAllowed &amp;&amp; google.colab.files &amp;&amp; google.colab.files.view) {
    for (const element of document.querySelectorAll(&#39;.filepath&#39;)) {
      element.style.display = &#39;block&#39;
      element.onclick = (event) =&gt; {
        event.preventDefault();
        event.stopPropagation();
        google.colab.files.view(element.textContent, 5818);
      };
    }
  }
  for (const element of document.querySelectorAll(&#39;.function-repr-contents&#39;)) {
    element.onclick = (event) =&gt; {
      event.preventDefault();
      event.stopPropagation();
      element.classList.toggle(&#39;function-repr-contents-collapsed&#39;);
    };
  }
  &lt;/script&gt;
  &lt;/div&gt;</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="co"># Tampilkan distribusi kelas untuk melihat imbalance</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Distribusi Kelas Emosi ---&quot;</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="bu">print</span>(df[<span class="st">&#39;label&#39;</span>].value_counts())</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a>sns.countplot(x<span class="op">=</span><span class="st">&#39;label&#39;</span>, data<span class="op">=</span>df, order<span class="op">=</span>df[<span class="st">&#39;label&#39;</span>].value_counts().index)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a>plt.title(<span class="st">&#39;Distribusi Emosi dalam Dataset&#39;</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Jumlah Tweet&#39;</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Emosi&#39;</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a>plt.tight_layout()</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true"></a>plt.savefig(<span class="st">&#39;emotion_distribution.png&#39;</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Grafik distribusi emosi disimpan sebagai &#39;emotion_distribution.png&#39;&quot;</span>)</span></code></pre></div>
<pre><code>--- Distribusi Kelas Emosi ---
label
anger      1101
happy      1017
sadness     997
fear        649
love        637
Name: count, dtype: int64
Grafik distribusi emosi disimpan sebagai &#39;emotion_distribution.png&#39;</code></pre>
<figure>
<img src="https://theonlineconverter.com/uploads/Emotion_Classification_on_Twitter_Data_1765866968_files/Emotion_Classification_on_Twitter_Data_1765866968_4_1.png" alt="" /><figcaption>png</figcaption>
</figure>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="co"># 1. PERSIAPAN KAMUS DAN STOPWORD UNTUK PREPROCESSING</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a><span class="co"># Muat kamus alay dan ubah menjadi dictionary</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a>alay_df <span class="op">=</span> pd.read_csv(<span class="st">&quot;/content/kamus_alay.csv&quot;</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a>alay_dict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(alay_df[<span class="st">&quot;slang&quot;</span>], alay_df[<span class="st">&quot;formal&quot;</span>]))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a><span class="co"># Daftar stopword yang Anda berikan</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a>stopwords_list_string <span class="op">=</span> <span class="st">&quot;&quot;&quot;</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a><span class="st">ada adalah adanya adapun agak agaknya agar akan akankah akhir akhiri akhirnya aku akulah amat amatlah anda andalah antar antara antaranya apa apaan apabila apakah apalagi apatah artinya asal asalkan atas atau ataukah ataupun awal awalnya bagai bagaikan bagaimana bagaimanakah bagaimanapun bagi bagian bahkan bahwa bahwasanya baik bakal bakalan balik banyak bapak baru bawah beberapa begini beginian beginikah beginilah begitu begitukah begitulah begitupun bekerja belakang belakangan belum belumlah benar benarkah benarlah berada berakhir berakhirlah berakhirnya berapa berapakah berapalah berapapun berarti berawal berbagai berdatangan beri berikan berikut berikutnya berjumlah berkali-kali berkata berkehendak berkeinginan berkenaan berlainan berlalu berlangsung berlebihan bermacam bermacam-macam bermaksud bermula bersama bersama-sama bersiap bersiap-siap bertanya bertanya-tanya berturut berturut-turut bertutur berujar berupa besar betul betulkah biasa biasanya bila bilakah bisa bisakah boleh bolehkah bolelah buat bukan bukankah bukanlah bukannya bulan bung cara caranya cukup cukupkah cukuplah cuma dahulu dalam dan dapat dari daripada datang dekat demi demikian demikianlah dengan depan di dia diakhiri diakhirinya dialah diantara diantaranya diberi diberikan diberikannya dibuat dibuatnya didapat didatangkan digunakan diibaratkan diibaratkannya diingat diingatkan diinginkan dijawab dijelaskan dijelaskannya dikarenakan dikatakan dikatakannya dikerjakan diketahui diketahuinya dikira dilakukan dilalui dilihat dimaksud dimaksudkan dimaksudkannya dimaksudnya diminta dimintai dimisalkan dimulai dimulailah dimulainya dimungkinkan dini dipastikan diperbuat diperbuatnya dipergunakan diperkirakan diperlihatkan diperlukan diperlukannya dipersoalkan dipertanyakan dipunyai diri dirinya disampaikan disebut disebutkan disebutkannya disini disinilah ditambahkan ditandaskan ditanya ditanyai ditanyakan ditegaskan ditujukan ditunjuk ditunjuki ditunjukkan ditunjukkannya ditunjuknya dituturkan dituturkannya diucapkan diucapkannya diungkapkan dong dua dulu empat enggak enggaknya entah entahlah guna gunakan hal hampir hanya hanyalah hari harus haruslah harusnya hendak hendaklah hendaknya hingga ia ialah ibarat ibaratkan ibaratnya ibu ikut ingat ingat-ingat ingin inginkah inginkan ini inikah inilah itu itukah itulah jadi jadilah jadinya jangan jangankan janganlah jauh jawab jawaban jawabnya jelas jelaskan jelaslah jelasnya jika jikalau juga jumlah jumlahnya justru kala kalau kalaulah kalaupun kalian kami kamilah kamu kamulah kan kapan kapankah kapanpun karena karenanya kasus kata katakan katakanlah katanya ke keadaan kebetulan kecil kedua keduanya keinginan kelamaan kelihatan kelihatannya kelima keluar kembali kemudian kemungkinan kemungkinannya kenapa kepada kepadanya kesampaian keseluruhan keseluruhannya keterlaluan ketika khususnya kini kinilah kira kira-kira kiranya kita kitalah kok kurang lagi lagian lah lain lainnya lalu lama lamanya lanjut lanjutnya lebih lewat lima luar macam maka makanya makin malah malahan mampu mampukah mana manakala manalagi masa masalah masalahnya masih masihkah masing masing-masing mau maupun melainkan melakukan melalui melihat melihatnya memang memastikan memberi memberikan membuat memerlukan memihak meminta memintakan memisalkan memperbuat mempergunakan memperkirakan memperlihatkan mempersiapkan mempersoalkan mempertanyakan mempunyai memulai memungkinkan menaiki menambahkan menandaskan menanti menanti-nanti menantikan menanya menanyai menanyakan mendapat mendapatkan mendatang mendatangi mendatangkan menegaskan mengakhiri mengapa mengatakan mengatakannya mengenai mengerjakan mengetahui menggunakan menghendaki mengibaratkan mengibaratkannya mengingat mengingatkan menginginkan mengira mengucapkan mengucapkannya mengungkapkan menjadi menjawab menjelaskan menuju menunjuk menunjuki menunjukkan menunjuknya menurut menuturkan menyampaikan menyangkut menyatakan menyebutkan menyeluruh menyiapkan merasa mereka merekalah merupakan meski meskipun meyakini meyakinkan minta mirip misal misalkan misalnya mula mulai mulailah mulanya mungkin mungkinkah nah naik namun nanti nantinya nyaris nyatanya oleh olehnya pada padahal padanya pak paling panjang pantas para pasti pastilah penting pentingnya per percuma perlu perlukah perlunya pernah persoalan pertama pertama-tama pertanyaan pertanyakan pihak pihaknya pukul pula pun punya rasa rasanya rata rupanya saat saatnya saja sajalah saling sama sama-sama sambil sampai sampai-sampai sampaikan sana sangat sangatlah satu saya sayalah se sebab sebabnya sebagai sebagaimana sebagainya sebagian sebaik sebaik-baiknya sebaiknya sebaliknya sebanyak sebegini sebegitu sebelum sebelumnya sebenarnya seberapa sebesar sebetulnya sebisanya sebuah sebut sebutlah sebutnya secara secukupnya sedang sedangkan sedemikian sedikit sedikitnya seenaknya segala segalanya segera seharusnya sehingga seingat sejak sejauh sejenak sejumlah sekadar sekadarnya sekali sekali-kali sekalian sekaligus sekalipun sekarang sekarang sekecil seketika sekiranya sekitar sekitarnya sekurang-kurangnya sekurangnya sela selain selaku selalu selama selama-lamanya selamanya selanjutnya seluruh seluruhnya semacam semakin semampu semampunya semasa semasih semata semata-mata semaunya sementara semisal semisalnya sempat semua semuanya semula sendiri sendirian sendirinya seolah seolah-olah seorang sepanjang sepantasnya sepantasnyalah seperlunya seperti sepertinya sepihak sering seringnya serta serupa sesaat sesama sesampai sesegera sesekali seseorang sesuatu sesuatunya sesudah sesudahnya setelah setempat setengah seterusnya setiap setiba setibanya setidak-tidaknya setidaknya setinggi seusai sewaktu siap siapa siapakah siapapun sini sinilah soal soalnya suatu sudah sudahkah sudahlah supaya tadi tadinya tahu tahun tak tambah tambahnya tampak tampaknya tandas tandasnya tanpa tanya tanyakan tanyanya tapi tegas tegasnya telah tempat tengah tentang tentu tentulah tentunya tepat terakhir terasa terbanyak terdahulu terdapat terdiri terhadap terhadapnya teringat teringat-ingat terjadi terjadilah terjadinya terkira terlalu terlebih terlihat termasuk ternyata tersampaikan tersebut tersebutlah tertentu tertuju terus terutama tetap tetapi tiap tiba tiba-tiba tidak tidakkah tidaklah tiga tinggi toh tunjuk turut tutur tuturnya ucap ucapnya ujar ujarnya umum umumnya ungkap ungkapnya untuk usah usai waduh wah wahai waktu waktunya walau walaupun wong yaitu yakin yakni yang</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true"></a><span class="st">&quot;&quot;&quot;</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true"></a><span class="co"># Ubah string menjadi set untuk pencarian yang lebih cepat</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true"></a>stopwords_set <span class="op">=</span> <span class="bu">set</span>(stopwords_list_string.strip().split())</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a><span class="co"># 2. FUNGSI PREPROCESSING</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true"></a><span class="kw">def</span> preprocess_tweet(tweet: <span class="bu">str</span>, alay_dict: <span class="bu">dict</span>, stopwords_set: <span class="bu">set</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true"></a>    <span class="co"># 1. Case Folding (mengubah semua huruf menjadi huruf kecil)</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true"></a>    tweet <span class="op">=</span> tweet.lower()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true"></a>    <span class="co"># 2. Cleansing (hapus URL, mention, hashtag, angka, dan tanda baca)</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true"></a>    tweet <span class="op">=</span> re.sub(<span class="vs">r&quot;\[url\]|\[username\]|#\w+|\d+&quot;</span>, <span class="st">&quot;&quot;</span>, tweet)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true"></a>    tweet <span class="op">=</span> re.sub(<span class="vs">r&quot;[^\w\s]&quot;</span>, <span class="st">&quot;&quot;</span>, tweet)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true"></a>    <span class="co"># 3. Tokenization (pemisahan kata)</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true"></a>    tokens <span class="op">=</span> tweet.split()</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true"></a>    <span class="co"># 4. Normalization and Stopword Removal</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true"></a>    processed_tokens <span class="op">=</span> []</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true"></a>    <span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true"></a>        <span class="co"># Normalisasi kata alay</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true"></a>        normalized_token <span class="op">=</span> alay_dict.get(token, token)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true"></a>        <span class="co"># Hapus stopwords</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true"></a>        <span class="cf">if</span> normalized_token <span class="kw">not</span> <span class="kw">in</span> stopwords_set:</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true"></a>            processed_tokens.append(normalized_token)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true"></a>    <span class="co"># Gabungkan kembali token menjadi kalimat</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true"></a>    <span class="cf">return</span> <span class="st">&quot; &quot;</span>.join(processed_tokens)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true"></a>df[<span class="st">&#39;cleaned_tweet&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;tweet&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: preprocess_tweet(x, alay_dict, stopwords_set))</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true"></a><span class="co"># Encode Label</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true"></a>y <span class="op">=</span> label_encoder.fit_transform(df[<span class="st">&#39;label&#39;</span>])</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a><span class="co"># 3. FEATURE EXTRACTION DENGAN TF-IDF</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer(ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>), min_df<span class="op">=</span><span class="dv">3</span>, max_df<span class="op">=</span><span class="fl">0.9</span>, max_features<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true"></a>X_tfidf <span class="op">=</span> tfidf_vectorizer.fit_transform(df[<span class="st">&#39;cleaned_tweet&#39;</span>])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true"></a><span class="co"># 4. PEMBAGIAN DATA</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true"></a>X_train_tfidf, X_test_tfidf, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true"></a>    X_tfidf, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true"></a>)</span></code></pre></div>
<p>Penjelasan: Dilakukan dua ekstraksi fitur, yaitu TF-IDF dan Word2Vec, untuk membuat perbandingan antara pemodelan yang menggunakan ekstraksi fitur yang tidak mempertimbangkan konteks dan makna (TF-IDF), serta pemodelan yang menggunakan ekstrakssi fitur yang memahami makna dan konteks (Word2vec).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="co"># 5. PEMBAGIAN DATA &amp; PENERAPAN SMOTE</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a>X_train_tfidf, X_test_tfidf, y_train, y_test <span class="op">=</span> train_test_split(X_tfidf, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true"></a>smote_tfidf <span class="op">=</span> SMOTE(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true"></a>X_train_resampled_tfidf, y_train_resampled_tfidf <span class="op">=</span> smote_tfidf.fit_resample(X_train_tfidf, y_train)</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a><span class="co"># 6. PEMODELAN DAN TUNING</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true"></a><span class="co"># 6a. SVM</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span> <span class="op">+</span> <span class="st">&quot;=&quot;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;MEMULAI TUNING UNTUK SVM (DATA SMOTE)&quot;</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;=&quot;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true"></a>param_grid_svm <span class="op">=</span> {<span class="st">&#39;C&#39;</span>: [<span class="dv">1</span>, <span class="dv">10</span>], <span class="st">&#39;kernel&#39;</span>: [<span class="st">&#39;linear&#39;</span>, <span class="st">&#39;rbf&#39;</span>]}</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true"></a>grid_search_svm <span class="op">=</span> GridSearchCV(SVC(random_state<span class="op">=</span><span class="dv">42</span>), param_grid_svm, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true"></a>grid_search_svm.fit(X_train_resampled_tfidf, y_train_resampled_tfidf)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true"></a>best_svm <span class="op">=</span> grid_search_svm.best_estimator_</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true"></a>y_pred_svm <span class="op">=</span> best_svm.predict(X_test_tfidf)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Hasil Terbaik SVM (Tuning + SMOTE) ---&quot;</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Parameter Terbaik: </span><span class="sc">{</span>grid_search_svm<span class="sc">.</span>best_params_<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true"></a><span class="bu">print</span>(classification_report(y_test, y_pred_svm, target_names<span class="op">=</span>label_encoder.classes_))</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true"></a><span class="co"># 6b. Logistic Regression</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span> <span class="op">+</span> <span class="st">&quot;=&quot;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;MEMULAI TUNING UNTUK LOGISTIC REGRESSION (DATA SMOTE)&quot;</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;=&quot;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true"></a>param_grid_lr <span class="op">=</span> {<span class="st">&#39;C&#39;</span>: [<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>], <span class="st">&#39;solver&#39;</span>: [<span class="st">&#39;liblinear&#39;</span>, <span class="st">&#39;saga&#39;</span>]}</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true"></a>grid_search_lr <span class="op">=</span> GridSearchCV(LogisticRegression(random_state<span class="op">=</span><span class="dv">42</span>, max_iter<span class="op">=</span><span class="dv">1000</span>), param_grid_lr, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true"></a>grid_search_lr.fit(X_train_resampled_tfidf, y_train_resampled_tfidf)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true"></a>best_lr <span class="op">=</span> grid_search_lr.best_estimator_</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true"></a>y_pred_lr <span class="op">=</span> best_lr.predict(X_test_tfidf)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Hasil Terbaik Logistic Regression (Tuning + SMOTE) ---&quot;</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Parameter Terbaik: </span><span class="sc">{</span>grid_search_lr<span class="sc">.</span>best_params_<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true"></a><span class="bu">print</span>(classification_report(y_test, y_pred_lr, target_names<span class="op">=</span>label_encoder.classes_))</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true"></a><span class="co"># 6c. Random Forest</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span> <span class="op">+</span> <span class="st">&quot;=&quot;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;MEMULAI TUNING UNTUK RANDOM FOREST (DATA SMOTE)&quot;</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;=&quot;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true"></a>param_grid_rf <span class="op">=</span> {<span class="st">&#39;n_estimators&#39;</span>: [<span class="dv">100</span>, <span class="dv">200</span>], <span class="st">&#39;max_depth&#39;</span>: [<span class="dv">50</span>, <span class="va">None</span>], <span class="st">&#39;min_samples_split&#39;</span>: [<span class="dv">2</span>, <span class="dv">5</span>]}</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true"></a>grid_search_rf <span class="op">=</span> GridSearchCV(RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>), param_grid_rf, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true"></a>grid_search_rf.fit(X_train_resampled_tfidf, y_train_resampled_tfidf)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true"></a>best_rf <span class="op">=</span> grid_search_rf.best_estimator_</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true"></a>y_pred_rf <span class="op">=</span> best_rf.predict(X_test_tfidf)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Hasil Terbaik Random Forest (Tuning + SMOTE) ---&quot;</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Parameter Terbaik: </span><span class="sc">{</span>grid_search_rf<span class="sc">.</span>best_params_<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true"></a><span class="bu">print</span>(classification_report(y_test, y_pred_rf, target_names<span class="op">=</span>label_encoder.classes_))</span></code></pre></div>
<pre><code>==================================================
MEMULAI TUNING UNTUK SVM (DATA SMOTE)
==================================================
Fitting 3 folds for each of 4 candidates, totalling 12 fits

--- Hasil Terbaik SVM (Tuning + SMOTE) ---
Parameter Terbaik: {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;rbf&#39;}
              precision    recall  f1-score   support

       anger       0.61      0.77      0.68       220
        fear       0.85      0.59      0.70       130
       happy       0.63      0.59      0.61       204
        love       0.88      0.60      0.71       127
     sadness       0.46      0.54      0.49       200

    accuracy                           0.62       881
   macro avg       0.68      0.62      0.64       881
weighted avg       0.65      0.62      0.63       881


==================================================
MEMULAI TUNING UNTUK LOGISTIC REGRESSION (DATA SMOTE)
==================================================
Fitting 3 folds for each of 6 candidates, totalling 18 fits

--- Hasil Terbaik Logistic Regression (Tuning + SMOTE) ---
Parameter Terbaik: {&#39;C&#39;: 10, &#39;solver&#39;: &#39;liblinear&#39;}
              precision    recall  f1-score   support

       anger       0.64      0.68      0.66       220
        fear       0.77      0.68      0.72       130
       happy       0.61      0.58      0.59       204
        love       0.81      0.69      0.74       127
     sadness       0.46      0.53      0.49       200

    accuracy                           0.62       881
   macro avg       0.66      0.63      0.64       881
weighted avg       0.64      0.62      0.63       881


==================================================
MEMULAI TUNING UNTUK RANDOM FOREST (DATA SMOTE)
==================================================
Fitting 3 folds for each of 8 candidates, totalling 24 fits

--- Hasil Terbaik Random Forest (Tuning + SMOTE) ---
Parameter Terbaik: {&#39;max_depth&#39;: None, &#39;min_samples_split&#39;: 5, &#39;n_estimators&#39;: 200}
              precision    recall  f1-score   support

       anger       0.53      0.70      0.60       220
        fear       0.83      0.68      0.75       130
       happy       0.66      0.51      0.58       204
        love       0.82      0.79      0.80       127
     sadness       0.44      0.45      0.44       200

    accuracy                           0.61       881
   macro avg       0.66      0.63      0.63       881
weighted avg       0.63      0.61      0.61       881</code></pre>
<p>Penjelasan: Dilakukan tuning grid search pada ketiga model agar dapat langsung mendapatkan parameter terbaik.</p>
<p>SVM mendapat akurasi 62% dan f1-skor average 0.64, ini menunjukkan jika model ini cukup balance dalam mengklasifikasikan tiap kelas. Bahkan untuk kelas minoritas seperti love dan fear mendapat precision sekitar 0.8 yang berarti model sering benar menebak kelas tersebut. Namun, recall untuk love dan fear masih rendah, yaitu sekitar 0.5-0.6, yang berarti model masih banyak melewatkan data dengan kelas tersebut. Selain itu, kelas sadness memiliki skor paling rendah di antara kelas lain, yang berarti model masih sulit mendeteksi kelas ini.</p>
<p>Logistic regression juga mendapat akurasi dan f1-score yang sama dengan SVM, yaitu 62% dan 0.64, ini menunjukkan bahwa model logistic regression ini juga seimbang dalam mengklasifikasikan kelas. Secara keseluruhan, precision dan recall di tiap kelas cukup seimbang dibandingkan dengan SVM.</p>
<p>Random forest mendapat akurasi 61% dan f1-score average 0.63, model ini mendapat akurasi paling rendah dibandingkan dengan kedua model lainnya. Namun, random forest mendapat f1-score yang paling tinggi untuk kelas love dan fear, menunjukkan bahwa model ini paling baik dalam mengklasifikasikan kelas minoritas. Namun, dalam mengklasifikasikan kelas mayoritas, model ini tidak begitu baik.</p>
<h3 id="kesimpulan-classification">Kesimpulan Classification</h3>
<p>Model klasifikasi biasa yang paling baik untuk dataset ini adalah logistic regression karena mendapat akurasi paling tinggi, yaitu 62%. Meskipun SVM juga mendapat akurasi yang sama, skor classification report logistic regression lebih seimbang di semua kelas daripada SVM.</p>
<p>Meski begitu, hasil pemodelan secara keseluruhan masih belum begitu baik dan optimal, sehingga perlu dilakukan percobaan model lain menggunakan deep learning.</p>
<h1 id="deep-learning">Deep Learning</h1>
<h2 id="d-cnn">1D CNN</h2>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true"></a><span class="im">import</span> re</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true"></a><span class="im">from</span> tensorflow.keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true"></a><span class="im">from</span> tensorflow.keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true"></a><span class="im">from</span> tensorflow.keras.callbacks <span class="im">import</span> EarlyStopping</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, accuracy_score</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> class_weight</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true"></a><span class="co"># 1. SETUP DAN PREPROCESSING</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true"></a><span class="co"># Muat data</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true"></a><span class="cf">try</span>:</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true"></a>    df <span class="op">=</span> pd.read_csv(<span class="st">&#39;Twitter_Emotion_Dataset.csv&#39;</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true"></a>    alay_df <span class="op">=</span> pd.read_csv(<span class="st">&#39;kamus_alay.csv&#39;</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true"></a>    alay_dict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(alay_df[<span class="st">&#39;slang&#39;</span>], alay_df[<span class="st">&#39;formal&#39;</span>]))</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;Dataset dan kamus alay berhasil dimuat.&quot;</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;File CSV tidak ditemukan.&quot;</span>)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true"></a>    exit()</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true"></a><span class="co"># Fungsi preprocessing (tidak menghapus stopwords)</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true"></a><span class="kw">def</span> preprocess_for_dl(tweet: <span class="bu">str</span>, alay_dict: <span class="bu">dict</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true"></a>    tweet <span class="op">=</span> <span class="bu">str</span>(tweet)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true"></a>    <span class="co"># 1. Case folding (mengubah semua huruf menjadi huruf kecil)</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true"></a>    tweet <span class="op">=</span> tweet.lower()</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true"></a>    <span class="co"># 2. Hapus noise (URL, username, hashtag, angka)</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true"></a>    tweet <span class="op">=</span> re.sub(<span class="vs">r&#39;\[url\]|\[username\]|#\w+|\d+&#39;</span>, <span class="st">&#39;&#39;</span>, tweet)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true"></a>    <span class="co"># 3. Hapus tanda baca</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true"></a>    tweet <span class="op">=</span> re.sub(<span class="vs">r&#39;[^\w\s]&#39;</span>, <span class="st">&#39;&#39;</span>, tweet)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true"></a>    <span class="co"># 4. Normalisasi kata alay</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true"></a>    tokens <span class="op">=</span> tweet.split()</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true"></a>    normalized_tokens <span class="op">=</span> [alay_dict.get(token, token) <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true"></a></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true"></a>    <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join(normalized_tokens)</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true"></a></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Memulai preprocessing yang disesuaikan untuk Deep Learning...&quot;</span>)</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true"></a>df[<span class="st">&#39;cleaned_tweet&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;tweet&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: preprocess_for_dl(x, alay_dict))</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Preprocessing selesai.&quot;</span>)</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true"></a></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true"></a><span class="co"># 2. PERSIAPAN DATA</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true"></a><span class="co"># Ekstraksi fitur dengan embedding</span></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true"></a>MAX_WORDS <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true"></a>MAX_LEN <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true"></a>EMBEDDING_DIM <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true"></a></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true"></a>tokenizer <span class="op">=</span> Tokenizer(num_words<span class="op">=</span>MAX_WORDS, lower<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true"></a>tokenizer.fit_on_texts(df[<span class="st">&#39;cleaned_tweet&#39;</span>].values)</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true"></a></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true"></a>X_seq <span class="op">=</span> tokenizer.texts_to_sequences(df[<span class="st">&#39;cleaned_tweet&#39;</span>].values)</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true"></a>X_pad <span class="op">=</span> pad_sequences(X_seq, maxlen<span class="op">=</span>MAX_LEN)</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true"></a>y <span class="op">=</span> label_encoder.fit_transform(df[<span class="st">&#39;label&#39;</span>])</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true"></a>word_index <span class="op">=</span> tokenizer.word_index</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true"></a></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true"></a></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true"></a><span class="co"># 3. IMBALANCE HANDLING DENGAN CLASS WEIGHT</span></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true"></a><span class="co"># Menghitung bobot untuk setiap kelas agar model memberi perhatian lebih pada kelas minoritas</span></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true"></a>class_weights_array <span class="op">=</span> class_weight.compute_class_weight(</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true"></a>    class_weight<span class="op">=</span><span class="st">&#39;balanced&#39;</span>,</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true"></a>    classes<span class="op">=</span>np.unique(y),</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true"></a>    y<span class="op">=</span>y</span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true"></a>)</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true"></a><span class="co"># Mengubahnya menjadi format dictionary yang bisa dibaca Keras</span></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true"></a>class_weights <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">enumerate</span>(class_weights_array))</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true"></a></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Class Weights yang akan digunakan untuk Imbalance Handling:&quot;</span>)</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true"></a><span class="bu">print</span>(class_weights)</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true"></a></span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true"></a></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true"></a><span class="co"># 4. MEMBANGUN MODEL 1D CNN</span></span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true"></a>model_cnn_final <span class="op">=</span> Sequential()</span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true"></a>model_cnn_final.add(Embedding(input_dim<span class="op">=</span><span class="bu">min</span>(MAX_WORDS, <span class="bu">len</span>(word_index) <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true"></a>                                output_dim<span class="op">=</span>EMBEDDING_DIM,</span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true"></a>                                input_length<span class="op">=</span>MAX_LEN))</span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true"></a>model_cnn_final.add(Conv1D(<span class="dv">128</span>, <span class="dv">5</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true"></a>model_cnn_final.add(GlobalMaxPooling1D())</span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true"></a>model_cnn_final.add(Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true"></a>model_cnn_final.add(Dropout(<span class="fl">0.5</span>))</span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true"></a>model_cnn_final.add(Dense(<span class="bu">len</span>(label_encoder.classes_), activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>))</span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true"></a></span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true"></a>model_cnn_final.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;sparse_categorical_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Arsitektur Model 1D CNN:&quot;</span>)</span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true"></a>model_cnn_final.summary()</span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true"></a></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true"></a></span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true"></a><span class="co"># 5. MEMBAGI DATA, MELATIH, DAN MENGEVALUASI</span></span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_pad, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y)</span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true"></a></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Memulai pelatihan model...&quot;</span>)</span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true"></a>callbacks <span class="op">=</span> [EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, patience<span class="op">=</span><span class="dv">3</span>, min_delta<span class="op">=</span><span class="fl">0.0001</span>)]</span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true"></a></span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true"></a>history_final <span class="op">=</span> model_cnn_final.fit(X_train, y_train,</span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true"></a>                                    epochs<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true"></a>                                    batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true"></a>                                    validation_data<span class="op">=</span>(X_test, y_test),</span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true"></a>                                    callbacks<span class="op">=</span>callbacks,</span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true"></a>                                    class_weight<span class="op">=</span>class_weights) <span class="co"># Menerapkan imbalance handling di sini</span></span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true"></a></span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Pelatihan selesai.&quot;</span>)</span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true"></a></span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true"></a><span class="co"># 6. EVALUASI</span></span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Hasil Evaluasi 1D CNN (dengan Penyesuaian) ---&quot;</span>)</span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true"></a>y_pred_probs <span class="op">=</span> model_cnn_final.predict(X_test)</span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true"></a>y_pred <span class="op">=</span> np.argmax(y_pred_probs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true"></a></span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Akurasi: </span><span class="sc">{</span>accuracy_score(y_test, y_pred)<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Laporan Klasifikasi:&quot;</span>)</span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true"></a><span class="bu">print</span>(classification_report(y_test, y_pred, target_names<span class="op">=</span>label_encoder.classes_))</span></code></pre></div>
<pre><code>Dataset dan kamus alay berhasil dimuat.

Memulai preprocessing yang disesuaikan untuk Deep Learning...
Preprocessing selesai.

Class Weights yang akan digunakan untuk Imbalance Handling:
{0: np.float64(0.7994550408719346), 1: np.float64(1.3562403697996919), 2: np.float64(0.8654867256637168), 3: np.float64(1.3817896389324962), 4: np.float64(0.8828485456369107)}

Arsitektur Model 1D CNN:


/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.
  warnings.warn(</code></pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)           │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv1D</span>)                 │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ global_max_pooling1d            │ ?                      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalMaxPooling1D</span>)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)               │ ?                      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
<pre><code>Memulai pelatihan model...
Epoch 1/15
[1m55/55[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m6s[0m 21ms/step - accuracy: 0.1927 - loss: 1.6133 - val_accuracy: 0.2770 - val_loss: 1.5255
Epoch 2/15
[1m55/55[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - accuracy: 0.4531 - loss: 1.3386 - val_accuracy: 0.5414 - val_loss: 1.2143
Epoch 3/15
[1m55/55[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - accuracy: 0.6649 - loss: 0.9189 - val_accuracy: 0.6129 - val_loss: 1.0154
Epoch 4/15
[1m55/55[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - accuracy: 0.8703 - loss: 0.5108 - val_accuracy: 0.6311 - val_loss: 0.9932
Epoch 5/15
[1m55/55[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - accuracy: 0.9694 - loss: 0.1776 - val_accuracy: 0.6243 - val_loss: 1.0527
Epoch 6/15
[1m55/55[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.9907 - loss: 0.0671 - val_accuracy: 0.6334 - val_loss: 1.1337
Epoch 7/15
[1m55/55[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.9959 - loss: 0.0389 - val_accuracy: 0.6322 - val_loss: 1.2320

Pelatihan selesai.

--- Hasil Evaluasi 1D CNN (dengan Penyesuaian) ---
[1m28/28[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 21ms/step
Akurasi: 0.6322

Laporan Klasifikasi:
              precision    recall  f1-score   support

       anger       0.65      0.66      0.66       220
        fear       0.82      0.66      0.73       130
       happy       0.64      0.52      0.57       204
        love       0.78      0.74      0.76       127
     sadness       0.47      0.62      0.53       200

    accuracy                           0.63       881
   macro avg       0.67      0.64      0.65       881
weighted avg       0.65      0.63      0.64       881</code></pre>
<p>Penjelasan: Model 1D CNN mendapat akurasi dan f1-score average yang sedikit lebih tinggi daripada model klasifikasi biasa, yaitu 63% dan 0.65, ini menunjukkan bahwa model deep learning ini mampu menangkap pola data yang lebih kompleks daripada model biasa.</p>
<p>Pada kelas love, precision dan recall-nya cukup tinggi dan seimbang, yaitu 0.7, yang berarti model cukup baik dalam mengklasifikasikan tweet love.</p>
<p>Pada kelas fear, precision cukup tinggi, yaitu 0.82, namun recall-nya cukup rendah, sehingga tidak seimbang.</p>
<p>Pada kelas anger dan happy, hasilnya sedikit lebih rendah dari love, namun masih cukup baik dibandingkan kelas sadness.</p>
<p>Pada kelas sadness, sama seperti model biasa, precision dan recall sangat rendah, yang berarti model masih belum dapat mengklasifikasikan tweet sedih dengan baik meskipun telah menggunakan deep learning.</p>
<h2 id="indobert">IndoBERT</h2>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true"></a><span class="im">import</span> os</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true"></a>os.environ[<span class="st">&quot;WANDB_DISABLED&quot;</span>] <span class="op">=</span> <span class="st">&quot;true&quot;</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true"></a><span class="im">import</span> re</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, classification_report</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> class_weight</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> RandomOverSampler</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true"></a><span class="co"># 1. MEMUAT DATA DAN PREPROCESSING</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true"></a><span class="cf">try</span>:</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true"></a>    df <span class="op">=</span> pd.read_csv(<span class="st">&#39;/content/Twitter_Emotion_Dataset.csv&#39;</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true"></a>    alay_df <span class="op">=</span> pd.read_csv(<span class="st">&#39;/content/kamus_alay.csv&#39;</span>)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true"></a>    alay_dict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(alay_df[<span class="st">&#39;slang&#39;</span>], alay_df[<span class="st">&#39;formal&#39;</span>]))</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;Dataset dan kamus alay berhasil dimuat.&quot;</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;File CSV tidak ditemukan.&quot;</span>)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true"></a>    exit()</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true"></a><span class="kw">def</span> preprocess_for_bert(tweet: <span class="bu">str</span>, alay_dict: <span class="bu">dict</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true"></a>    tweet <span class="op">=</span> <span class="bu">str</span>(tweet).lower() <span class="co"># Case folding (mengubah emua huruf menjadi huruf kecil)</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true"></a>    tweet <span class="op">=</span> re.sub(<span class="vs">r&#39;\[url\]|\[username\]|#\w+|\d+&#39;</span>, <span class="st">&#39;&#39;</span>, tweet) <span class="co"># Cleansing (hapus URL, username, hashtag, angka)</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true"></a>    tweet <span class="op">=</span> re.sub(<span class="vs">r&#39;[^\w\s]&#39;</span>, <span class="st">&#39;&#39;</span>, tweet) <span class="co"># Hapus tanda baca</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true"></a>    tokens <span class="op">=</span> tweet.split()</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true"></a>    normalized_tokens <span class="op">=</span> [alay_dict.get(token, token) <span class="cf">for</span> token <span class="kw">in</span> tokens] <span class="co"># Normalisasi kata alay/slang</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true"></a>    <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join(normalized_tokens)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true"></a>df[<span class="st">&#39;cleaned_tweet&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;tweet&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: preprocess_for_bert(x, alay_dict))</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true"></a>labels <span class="op">=</span> <span class="bu">sorted</span>(df[<span class="st">&#39;label&#39;</span>].unique().tolist())</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true"></a>label_map <span class="op">=</span> {label: i <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(labels)}</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true"></a>df[<span class="st">&#39;label_encoded&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;label&#39;</span>].<span class="bu">map</span>(label_map)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true"></a></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true"></a><span class="co"># 2. MEMBAGI DATA DAN TOKENISASI</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true"></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true"></a>    df[<span class="st">&#39;cleaned_tweet&#39;</span>].tolist(),</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true"></a>    df[<span class="st">&#39;label_encoded&#39;</span>].tolist(),</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true"></a>    stratify<span class="op">=</span>df[<span class="st">&#39;label_encoded&#39;</span>].tolist()</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true"></a>)</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true"></a>model_name <span class="op">=</span> <span class="st">&#39;indobenchmark/indobert-base-p1&#39;</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true"></a></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true"></a>train_encodings <span class="op">=</span> tokenizer(X_train, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true"></a>val_encodings <span class="op">=</span> tokenizer(X_val, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true"></a></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true"></a><span class="kw">class</span> EmotionDataset(torch.utils.data.Dataset):</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encodings, labels):</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true"></a>        <span class="va">self</span>.encodings <span class="op">=</span> encodings</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true"></a>        item <span class="op">=</span> {key: torch.tensor(val[idx]) <span class="cf">for</span> key, val <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true"></a>        item[<span class="st">&#39;labels&#39;</span>] <span class="op">=</span> torch.tensor(<span class="va">self</span>.labels[idx])</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true"></a>        <span class="cf">return</span> item</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true"></a></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true"></a>train_dataset <span class="op">=</span> EmotionDataset(train_encodings, y_train)</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true"></a>val_dataset <span class="op">=</span> EmotionDataset(val_encodings, y_val)</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true"></a></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true"></a><span class="co"># 3. OVERSAMPLING DATA LATIH</span></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Menerapkan Random Oversampling pada Data Latih ---&quot;</span>)</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true"></a><span class="co"># Resampler hanya bekerja pada data numerik, jadi kita reshape data teks</span></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true"></a>ros <span class="op">=</span> RandomOverSampler(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true"></a>X_train_reshaped <span class="op">=</span> np.array(X_train).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true"></a>X_train_resampled, y_train_resampled <span class="op">=</span> ros.fit_resample(X_train_reshaped, y_train)</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true"></a></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true"></a><span class="co"># Kembalikan X_train ke bentuk list 1D</span></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true"></a>X_train_resampled <span class="op">=</span> X_train_resampled.flatten().tolist()</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true"></a></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Ukuran data latih sebelum oversampling: </span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Ukuran data latih setelah oversampling: </span><span class="sc">{</span><span class="bu">len</span>(X_train_resampled)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true"></a></span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true"></a></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true"></a><span class="co"># 4. TOKENISASI DAN MEMBUAT DATASET</span></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true"></a>model_name <span class="op">=</span> <span class="st">&#39;indobenchmark/indobert-base-p1&#39;</span></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true"></a></span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true"></a><span class="co"># Tokenisasi data latih yang sudah di-oversample</span></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true"></a>train_encodings <span class="op">=</span> tokenizer(X_train_resampled, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true"></a><span class="co"># Tokenisasi data validasi yang tidak di-oversample</span></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true"></a>val_encodings <span class="op">=</span> tokenizer(X_val, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true"></a></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true"></a><span class="kw">class</span> EmotionDataset(torch.utils.data.Dataset):</span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encodings, labels):</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true"></a>        <span class="va">self</span>.encodings <span class="op">=</span> encodings</span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true"></a>        item <span class="op">=</span> {key: torch.tensor(val[idx]) <span class="cf">for</span> key, val <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true"></a>        item[<span class="st">&#39;labels&#39;</span>] <span class="op">=</span> torch.tensor(<span class="va">self</span>.labels[idx])</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true"></a>        <span class="cf">return</span> item</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true"></a></span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true"></a>train_dataset <span class="op">=</span> EmotionDataset(train_encodings, y_train_resampled)</span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true"></a>val_dataset <span class="op">=</span> EmotionDataset(val_encodings, y_val)</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true"></a></span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true"></a></span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true"></a><span class="co"># 5. FINE-TUNING DENGAN TRAINER STANDAR</span></span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_name, num_labels<span class="op">=</span><span class="bu">len</span>(labels))</span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true"></a></span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true"></a><span class="kw">def</span> compute_metrics(pred):</span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true"></a>    labels <span class="op">=</span> pred.label_ids</span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true"></a>    preds <span class="op">=</span> pred.predictions.argmax(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true"></a>    accuracy <span class="op">=</span> accuracy_score(labels, preds)</span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true"></a>    <span class="cf">return</span> {<span class="st">&#39;accuracy&#39;</span>: accuracy}</span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true"></a></span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true"></a>    output_dir<span class="op">=</span><span class="st">&#39;./results_oversampled&#39;</span>,</span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true"></a>    num_train_epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true"></a>    learning_rate<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true"></a>    logging_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true"></a>    eval_strategy<span class="op">=</span><span class="st">&quot;epoch&quot;</span>,</span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true"></a>    save_strategy<span class="op">=</span><span class="st">&quot;epoch&quot;</span>,</span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true"></a>)</span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true"></a></span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true"></a>    model<span class="op">=</span>model,</span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true"></a>    eval_dataset<span class="op">=</span>val_dataset,</span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true"></a>    compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb18-132"><a href="#cb18-132" aria-hidden="true"></a>)</span>
<span id="cb18-133"><a href="#cb18-133" aria-hidden="true"></a></span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Memulai proses fine-tuning IndoBERT dengan Data Hasil Oversampling...&quot;</span>)</span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true"></a>trainer.train()</span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Fine-tuning selesai.&quot;</span>)</span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true"></a></span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true"></a></span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true"></a><span class="co"># 6. EVALUASI</span></span>
<span id="cb18-140"><a href="#cb18-140" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Mengevaluasi model terbaik pada data validasi ---&quot;</span>)</span>
<span id="cb18-141"><a href="#cb18-141" aria-hidden="true"></a>predictions <span class="op">=</span> trainer.predict(val_dataset)</span>
<span id="cb18-142"><a href="#cb18-142" aria-hidden="true"></a>preds <span class="op">=</span> np.argmax(predictions.predictions, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-143"><a href="#cb18-143" aria-hidden="true"></a></span>
<span id="cb18-144"><a href="#cb18-144" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Laporan Klasifikasi Final (dengan Oversampling):&quot;</span>)</span>
<span id="cb18-145"><a href="#cb18-145" aria-hidden="true"></a><span class="bu">print</span>(classification_report(y_val, preds, target_names<span class="op">=</span>labels))</span></code></pre></div>
<pre><code>Dataset dan kamus alay berhasil dimuat.


/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(



tokenizer_config.json:   0%|          | 0.00/2.00 [00:00&lt;?, ?B/s]



config.json: 0.00B [00:00, ?B/s]



vocab.txt: 0.00B [00:00, ?B/s]



special_tokens_map.json:   0%|          | 0.00/112 [00:00&lt;?, ?B/s]



--- Menerapkan Random Oversampling pada Data Latih ---
Ukuran data latih sebelum oversampling: 3520
Ukuran data latih setelah oversampling: 4405



pytorch_model.bin:   0%|          | 0.00/498M [00:00&lt;?, ?B/s]


Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).



model.safetensors:   0%|          | 0.00/498M [00:00&lt;?, ?B/s]



Memulai proses fine-tuning IndoBERT dengan Data Hasil Oversampling...




&lt;div&gt;

  &lt;progress value=&#39;1380&#39; max=&#39;1380&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  [1380/1380 09:18, Epoch 5/5]
&lt;/div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;</code></pre>
<thead>
<tr style="text-align: left;">
<th>
Epoch
</th>
<th>
Training Loss
</th>
<th>
Validation Loss
</th>
<th>
Accuracy
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
0.741900
</td>
<td>
0.761239
</td>
<td>
0.725312
</td>
</tr>
<tr>
<td>
2
</td>
<td>
0.497900
</td>
<td>
0.802850
</td>
<td>
0.702611
</td>
</tr>
<tr>
<td>
3
</td>
<td>
0.285500
</td>
<td>
0.864567
</td>
<td>
0.723042
</td>
</tr>
<tr>
<td>
4
</td>
<td>
0.177400
</td>
<td>
0.957503
</td>
<td>
0.729852
</td>
</tr>
<tr>
<td>
5
</td>
<td>
0.100100
</td>
<td>
1.012767
</td>
<td>
0.724177
</td>
</tr>
</tbody>
</table>
<p>
<pre><code>Fine-tuning selesai.

--- Mengevaluasi model terbaik pada data validasi ---







Laporan Klasifikasi Final (dengan Oversampling):
              precision    recall  f1-score   support

       anger       0.77      0.72      0.74       220
        fear       0.76      0.75      0.76       130
       happy       0.81      0.69      0.74       204
        love       0.82      0.85      0.84       127
     sadness       0.56      0.67      0.61       200

    accuracy                           0.73       881
   macro avg       0.74      0.74      0.74       881
weighted avg       0.74      0.73      0.73       881</code></pre>
<p>Penjelasan: Model IndoBERT mendapat akurasi 73% dan f1-score average 0.74, peningkatan skor ini cukup signifikan dibanding model-model sebelumnya.</p>
<p>Pada kelas love, lagi-lagi mendapat skor paling tinggi di antara kelas lain pada precision dan recall, yaitu sekitar 0.8, yang berarti model sangat baik dalam mendeteksi emosi ini. Begitu juga kelas happy yang mendapat akurasi sedikit di bawah love, 0.81, namun tidak seimbang dengan recall-nya yang 0.69.</p>
<p>Pada kelas fear dan anger juga mengalami peningkatan skor meskipun tidak setinggi love dan happy, namun cukup seimbang di antara precision dan recall-nya.</p>
<p>Pada kelas sadness, skornya juga ikut meningkat, ini karena dilakukan random oversampling sehingga model dapat belajar lebih banyak mengenai data sadness. Meskipun precision-nya masih 0.55, yang berarti model masih sering salah menebak, setidaknya recall 0.7 menunjukkan model sudah lebih berani mendeteksi sadness.</p>
<h2 id="indobert-dengan-tambahan-data-baru">IndoBERT (Dengan Tambahan Data Baru)</h2>
<p>Menggunakan data tambahan IndoNLU EmoT Dataset dari Hugging Face (https://huggingface.co/datasets/indonlp/indonlu)</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true"></a>pip install transformers torch scikit<span class="op">-</span>learn pandas imbalanced<span class="op">-</span>learn datasets</span></code></pre></div>
<pre><code>Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)
Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)
Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)
Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)
Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)
Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)
Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)
Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)
Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)
Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers&lt;=0.23.0,&gt;=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)
Requirement already satisfied: safetensors&gt;=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)
Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)
Requirement already satisfied: typing-extensions&gt;=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)
Requirement already satisfied: sympy&gt;=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)
Requirement already satisfied: networkx&gt;=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec&gt;=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)
Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)
Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)
Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)
Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)
Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)
Requirement already satisfied: scipy&gt;=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)
Requirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)
Requirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)
Requirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)
Requirement already satisfied: pyarrow&gt;=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)
Requirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)
Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)
Requirement already satisfied: multiprocess&lt;0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (3.13.2)
Requirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers) (1.2.0)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)
Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;transformers) (3.4.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;transformers) (3.11)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;transformers) (2.5.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;transformers) (2025.11.12)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy&gt;=1.13.3-&gt;torch) (1.3.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-&gt;torch) (3.0.3)
Requirement already satisfied: aiohappyeyeballs&gt;=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (2.6.1)
Requirement already satisfied: aiosignal&gt;=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (1.4.0)
Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (25.4.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (1.8.0)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (6.7.0)
Requirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (0.4.1)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (1.22.0)</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true"></a><span class="im">import</span> re</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, classification_report</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> RandomOverSampler</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true"></a><span class="co"># 1. MEMUAT DAN MENGGABUNGKAN SEMUA DATA</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true"></a><span class="cf">try</span>:</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true"></a>    <span class="co"># Muat semua dataset</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true"></a>    df_lama <span class="op">=</span> pd.read_csv(<span class="st">&quot;/content/Twitter_Emotion_Dataset.csv&quot;</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true"></a>    df_baru_train <span class="op">=</span> pd.read_csv(<span class="st">&quot;/content/train_preprocess.csv&quot;</span>)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true"></a>    df_baru_valid <span class="op">=</span> pd.read_csv(<span class="st">&quot;/content/valid_preprocess.csv&quot;</span>)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;Berhasil memuat 3 file dataset.&quot;</span>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true"></a>    <span class="co"># Gabungkan ketiga DataFrame menjadi satu</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true"></a>    df_gabungan <span class="op">=</span> pd.concat([df_lama, df_baru_train, df_baru_valid], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Ukuran dataset gabungan: </span><span class="sc">{</span><span class="bu">len</span>(df_gabungan)<span class="sc">}</span><span class="ss"> baris&quot;</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Distribusi label pada dataset gabungan:&quot;</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true"></a>    <span class="bu">print</span>(df_gabungan[<span class="st">&#39;label&#39;</span>].value_counts())</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span> <span class="im">as</span> e:</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Error: File tidak ditemukan. Pastikan nama file sudah benar. Detail: </span><span class="sc">{e}</span><span class="ss">&quot;</span>)</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true"></a>    exit()</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true"></a><span class="co"># 2. PREPROCESSING</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true"></a><span class="cf">try</span>:</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true"></a>    alay_df <span class="op">=</span> pd.read_csv(<span class="st">&#39;kamus_alay.csv&#39;</span>)</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true"></a>    alay_dict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(alay_df[<span class="st">&#39;slang&#39;</span>], alay_df[<span class="st">&#39;formal&#39;</span>]))</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Kamus alay berhasil dimuat.&quot;</span>)</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;Peringatan: kamus_alay.csv tidak ditemukan. Normalisasi kata alay tidak akan dilakukan.&quot;</span>)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true"></a>    alay_dict <span class="op">=</span> {}</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true"></a></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true"></a><span class="co"># Fungsi preprocessing</span></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true"></a><span class="kw">def</span> preprocess_for_bert(tweet: <span class="bu">str</span>, alay_dict: <span class="bu">dict</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true"></a>    tweet <span class="op">=</span> <span class="bu">str</span>(tweet)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true"></a>    tweet <span class="op">=</span> tweet.lower() <span class="co"># Case folding</span></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true"></a>    tweet <span class="op">=</span> re.sub(<span class="vs">r&#39;\[url\]|\[username\]|#\w+|\d+&#39;</span>, <span class="st">&#39;&#39;</span>, tweet) <span class="co"># Cleansing</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true"></a>    tweet <span class="op">=</span> re.sub(<span class="vs">r&#39;[^\w\s]&#39;</span>, <span class="st">&#39;&#39;</span>, tweet) <span class="co"># Hapus tanda baca</span></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true"></a>    tokens <span class="op">=</span> tweet.split()</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true"></a>    normalized_tokens <span class="op">=</span> [alay_dict.get(token, token) <span class="cf">for</span> token <span class="kw">in</span> tokens] <span class="co"># Normalisasi kata alay/slang</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true"></a>    <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join(normalized_tokens)</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true"></a></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Memulai preprocessing pada data gabungan...&quot;</span>)</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true"></a>df_gabungan[<span class="st">&#39;cleaned_tweet&#39;</span>] <span class="op">=</span> df_gabungan[<span class="st">&#39;tweet&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: preprocess_for_bert(x, alay_dict))</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true"></a></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true"></a><span class="co"># Mengubah label teks menjadi angka</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true"></a>labels <span class="op">=</span> <span class="bu">sorted</span>(df_gabungan[<span class="st">&#39;label&#39;</span>].unique().tolist())</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true"></a>label_map <span class="op">=</span> {label: i <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(labels)}</span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true"></a>df_gabungan[<span class="st">&#39;label_encoded&#39;</span>] <span class="op">=</span> df_gabungan[<span class="st">&#39;label&#39;</span>].<span class="bu">map</span>(label_map)</span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Preprocessing selesai.&quot;</span>)</span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true"></a></span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true"></a><span class="co"># 3. MEMBAGI DATA DAN OVERSAMPLING</span></span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true"></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(</span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true"></a>    df_gabungan[<span class="st">&#39;cleaned_tweet&#39;</span>].tolist(),</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true"></a>    df_gabungan[<span class="st">&#39;label_encoded&#39;</span>].tolist(),</span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, <span class="co"># Tetap menggunakan 20% dari data gabungan sebagai validation set</span></span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true"></a>    stratify<span class="op">=</span>df_gabungan[<span class="st">&#39;label_encoded&#39;</span>].tolist()</span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true"></a>)</span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true"></a></span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Menerapkan Random Oversampling pada Data Latih ---&quot;</span>)</span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true"></a>ros <span class="op">=</span> RandomOverSampler(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true"></a>X_train_reshaped <span class="op">=</span> np.array(X_train).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true"></a>X_train_resampled, y_train_resampled <span class="op">=</span> ros.fit_resample(X_train_reshaped, y_train)</span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true"></a>X_train_resampled <span class="op">=</span> X_train_resampled.flatten().tolist()</span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Ukuran data latih setelah oversampling: </span><span class="sc">{</span><span class="bu">len</span>(X_train_resampled)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true"></a></span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true"></a><span class="co"># 4. TOKENISASI DAN MEMBUAT DATASET</span></span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true"></a>model_name <span class="op">=</span> <span class="st">&#39;indobenchmark/indobert-base-p1&#39;</span></span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true"></a></span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true"></a>train_encodings <span class="op">=</span> tokenizer(X_train_resampled, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true"></a>val_encodings <span class="op">=</span> tokenizer(X_val, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true"></a></span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true"></a><span class="kw">class</span> EmotionDataset(torch.utils.data.Dataset):</span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encodings, labels):</span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true"></a>        <span class="va">self</span>.encodings <span class="op">=</span> encodings</span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true"></a>        item <span class="op">=</span> {key: torch.tensor(val[idx]) <span class="cf">for</span> key, val <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true"></a>        item[<span class="st">&#39;labels&#39;</span>] <span class="op">=</span> torch.tensor(<span class="va">self</span>.labels[idx])</span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true"></a>        <span class="cf">return</span> item</span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true"></a></span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true"></a>train_dataset <span class="op">=</span> EmotionDataset(train_encodings, y_train_resampled)</span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true"></a>val_dataset <span class="op">=</span> EmotionDataset(val_encodings, y_val)</span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true"></a></span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true"></a><span class="co"># 5. FINE-TUNING INDOBERT DENGAN SETELAN TERBAIK</span></span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_name, num_labels<span class="op">=</span><span class="bu">len</span>(labels))</span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true"></a></span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true"></a><span class="kw">def</span> compute_metrics(pred):</span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true"></a>    labels <span class="op">=</span> pred.label_ids</span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true"></a>    preds <span class="op">=</span> pred.predictions.argmax(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true"></a>    accuracy <span class="op">=</span> accuracy_score(labels, preds)</span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true"></a>    <span class="cf">return</span> {<span class="st">&#39;accuracy&#39;</span>: accuracy}</span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true"></a></span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb23-106"><a href="#cb23-106" aria-hidden="true"></a>    output_dir<span class="op">=</span><span class="st">&#39;./results_final&#39;</span>,</span>
<span id="cb23-107"><a href="#cb23-107" aria-hidden="true"></a>    num_train_epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb23-108"><a href="#cb23-108" aria-hidden="true"></a>    learning_rate<span class="op">=</span><span class="fl">1e-5</span>, <span class="co"># Learning rate rendah</span></span>
<span id="cb23-109"><a href="#cb23-109" aria-hidden="true"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb23-110"><a href="#cb23-110" aria-hidden="true"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb23-111"><a href="#cb23-111" aria-hidden="true"></a>    logging_steps<span class="op">=</span><span class="dv">100</span>, <span class="co"># Disesuaikan karena data latih lebih besar</span></span>
<span id="cb23-112"><a href="#cb23-112" aria-hidden="true"></a>    eval_strategy<span class="op">=</span><span class="st">&quot;epoch&quot;</span>,</span>
<span id="cb23-113"><a href="#cb23-113" aria-hidden="true"></a>    save_strategy<span class="op">=</span><span class="st">&quot;epoch&quot;</span>,</span>
<span id="cb23-114"><a href="#cb23-114" aria-hidden="true"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-115"><a href="#cb23-115" aria-hidden="true"></a>)</span>
<span id="cb23-116"><a href="#cb23-116" aria-hidden="true"></a></span>
<span id="cb23-117"><a href="#cb23-117" aria-hidden="true"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb23-118"><a href="#cb23-118" aria-hidden="true"></a>    model<span class="op">=</span>model,</span>
<span id="cb23-119"><a href="#cb23-119" aria-hidden="true"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb23-120"><a href="#cb23-120" aria-hidden="true"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb23-121"><a href="#cb23-121" aria-hidden="true"></a>    eval_dataset<span class="op">=</span>val_dataset,</span>
<span id="cb23-122"><a href="#cb23-122" aria-hidden="true"></a>    compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb23-123"><a href="#cb23-123" aria-hidden="true"></a>)</span>
<span id="cb23-124"><a href="#cb23-124" aria-hidden="true"></a></span>
<span id="cb23-125"><a href="#cb23-125" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Memulai proses fine-tuning IndoBERT pada DATA GABUNGAN...&quot;</span>)</span>
<span id="cb23-126"><a href="#cb23-126" aria-hidden="true"></a>trainer.train()</span>
<span id="cb23-127"><a href="#cb23-127" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Fine-tuning selesai.&quot;</span>)</span>
<span id="cb23-128"><a href="#cb23-128" aria-hidden="true"></a></span>
<span id="cb23-129"><a href="#cb23-129" aria-hidden="true"></a><span class="co"># 6. EVALUASI</span></span>
<span id="cb23-130"><a href="#cb23-130" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">--- Mengevaluasi model terbaik pada data validasi ---&quot;</span>)</span>
<span id="cb23-131"><a href="#cb23-131" aria-hidden="true"></a>predictions <span class="op">=</span> trainer.predict(val_dataset)</span>
<span id="cb23-132"><a href="#cb23-132" aria-hidden="true"></a>preds <span class="op">=</span> np.argmax(predictions.predictions, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb23-133"><a href="#cb23-133" aria-hidden="true"></a></span>
<span id="cb23-134"><a href="#cb23-134" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Laporan Klasifikasi Final (pada Data Gabungan):&quot;</span>)</span>
<span id="cb23-135"><a href="#cb23-135" aria-hidden="true"></a><span class="bu">print</span>(classification_report(y_val, preds, target_names<span class="op">=</span>labels))</span>
<span id="cb23-136"><a href="#cb23-136" aria-hidden="true"></a></span>
<span id="cb23-137"><a href="#cb23-137" aria-hidden="true"></a><span class="co"># 7. CONFUSION MATRIX</span></span>
<span id="cb23-138"><a href="#cb23-138" aria-hidden="true"></a>cm <span class="op">=</span> confusion_matrix(y_val, preds)</span>
<span id="cb23-139"><a href="#cb23-139" aria-hidden="true"></a></span>
<span id="cb23-140"><a href="#cb23-140" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb23-141"><a href="#cb23-141" aria-hidden="true"></a>sns.heatmap(</span>
<span id="cb23-142"><a href="#cb23-142" aria-hidden="true"></a>    cm,</span>
<span id="cb23-143"><a href="#cb23-143" aria-hidden="true"></a>    annot<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-144"><a href="#cb23-144" aria-hidden="true"></a>    fmt<span class="op">=</span><span class="st">&quot;d&quot;</span>,</span>
<span id="cb23-145"><a href="#cb23-145" aria-hidden="true"></a>    cmap<span class="op">=</span><span class="st">&quot;Blues&quot;</span>,</span>
<span id="cb23-146"><a href="#cb23-146" aria-hidden="true"></a>    xticklabels<span class="op">=</span>labels,</span>
<span id="cb23-147"><a href="#cb23-147" aria-hidden="true"></a>    yticklabels<span class="op">=</span>labels</span>
<span id="cb23-148"><a href="#cb23-148" aria-hidden="true"></a>)</span>
<span id="cb23-149"><a href="#cb23-149" aria-hidden="true"></a></span>
<span id="cb23-150"><a href="#cb23-150" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;Predicted Label&quot;</span>)</span>
<span id="cb23-151"><a href="#cb23-151" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;True Label&quot;</span>)</span>
<span id="cb23-152"><a href="#cb23-152" aria-hidden="true"></a>plt.title(<span class="st">&quot;Confusion Matrix - IndoBERT Emotion Classification&quot;</span>)</span>
<span id="cb23-153"><a href="#cb23-153" aria-hidden="true"></a>plt.tight_layout()</span>
<span id="cb23-154"><a href="#cb23-154" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<pre><code>Berhasil memuat 3 file dataset.

Ukuran dataset gabungan: 8362 baris

Distribusi label pada dataset gabungan:
label
anger      2092
happy      1933
sadness    1894
fear       1233
love       1210
Name: count, dtype: int64

Kamus alay berhasil dimuat.
Memulai preprocessing pada data gabungan...
Preprocessing selesai.

--- Menerapkan Random Oversampling pada Data Latih ---
Ukuran data latih setelah oversampling: 8370


Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).



Memulai proses fine-tuning IndoBERT pada DATA GABUNGAN...




&lt;div&gt;

  &lt;progress value=&#39;1601&#39; max=&#39;2620&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  [1601/2620 10:17 &lt; 06:33, 2.59 it/s, Epoch 3.05/5]
&lt;/div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;</code></pre>
<thead>
<tr style="text-align: left;">
<th>
Epoch
</th>
<th>
Training Loss
</th>
<th>
Validation Loss
</th>
<th>
Accuracy
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
0.541900
</td>
<td>
0.538228
</td>
<td>
0.805140
</td>
</tr>
<tr>
<td>
2
</td>
<td>
0.255300
</td>
<td>
0.349755
</td>
<td>
0.894202
</td>
</tr>
<tr>
<td>
3
</td>
<td>
0.094300
</td>
<td>
0.335877
</td>
<td>
0.918709
</td>
</tr>
</tbody>
</table>
<p>
<pre><code>&lt;div&gt;

  &lt;progress value=&#39;2620&#39; max=&#39;2620&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  [2620/2620 16:12, Epoch 5/5]
&lt;/div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;</code></pre>
<thead>
<tr style="text-align: left;">
<th>
Epoch
</th>
<th>
Training Loss
</th>
<th>
Validation Loss
</th>
<th>
Accuracy
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
0.541900
</td>
<td>
0.538228
</td>
<td>
0.805140
</td>
</tr>
<tr>
<td>
2
</td>
<td>
0.255300
</td>
<td>
0.349755
</td>
<td>
0.894202
</td>
</tr>
<tr>
<td>
3
</td>
<td>
0.094300
</td>
<td>
0.335877
</td>
<td>
0.918709
</td>
</tr>
<tr>
<td>
4
</td>
<td>
0.047000
</td>
<td>
0.314701
</td>
<td>
0.929468
</td>
</tr>
<tr>
<td>
5
</td>
<td>
0.028900
</td>
<td>
0.313632
</td>
<td>
0.931261
</td>
</tr>
</tbody>
</table>
<p>
<pre><code>Fine-tuning selesai.

--- Mengevaluasi model terbaik pada data validasi ---







Laporan Klasifikasi Final (pada Data Gabungan):
              precision    recall  f1-score   support

       anger       0.93      0.96      0.94       418
        fear       0.94      0.93      0.93       247
       happy       0.94      0.95      0.95       387
        love       0.97      0.96      0.96       242
     sadness       0.89      0.86      0.88       379

    accuracy                           0.93      1673
   macro avg       0.93      0.93      0.93      1673
weighted avg       0.93      0.93      0.93      1673</code></pre>
<figure>
<img src="https://theonlineconverter.com/uploads/Emotion_Classification_on_Twitter_Data_1765866968_files/Emotion_Classification_on_Twitter_Data_1765866968_24_8.png" alt="" /><figcaption>png</figcaption>
</figure>
<p>Penjelasan: Ditambahkan data baru sebanyak sekitar 4000 data, sehingga menghasilkan data gabungan sebanyak 8362 data. Penambahan data ini dilakukan agar model dapat belajar dengan lebih maksimal menggunakan data yang mengandung informasi baru mengenai karakteristik masing-masing emosi.</p>
<p>Hasilnya, akurasi model ini mencapai 93%, meningkat sangat signifikan dari model sebelumnya tanpa penambahan data.</p>
<p>Semua kelas mendapatkan skor 0.9, kecuali sadness yang hanya sektar 0.8, tetapi itu juga peningkatan yang sangat tinggi dibandingkan sebelumnya.</p>
<h3 id="kesimpulan">Kesimpulan</h3>
<p>Model terbaik untuk mengklasifikasikan emosi pada dataset ini adalah IndoBERT dengan menggunakan random oversampling dan dengan menambah data baru, serta tuning model dengan learning rate rendah dan epoch yang diperbanyak. Model ini mencapai akurasi 93% dengan skor keseluruhan yang seimbang pada semua kelas, serta tidak terjadi overfitting.</p>
<p>Ketika digunakan model IndoBERT tanpa penambahan data baru, akurasinya mencapai 73%, yang mana juga paling tinggi di antara model lain, yang berarti model ini merupakan model yang kinerjanya paling optimal. Oleh karena itu, diputuskan untuk menambahkan data baru menggunakan IndoBERT, agar model dapat lebih banyak mempelajari pola data yang kompleks, sehingga model dapat lebih memahami karakteristik tiap label.</p>
