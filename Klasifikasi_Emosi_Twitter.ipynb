{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azaliafitriana1/Emotion-Classification-on-Twitter-Data/blob/main/Klasifikasi_Emosi_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MbcLq5eLbLV_",
      "metadata": {
        "id": "MbcLq5eLbLV_"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v7LsWqaq7dj",
        "outputId": "9f8fb877-5b64-4000-918d-0db271f56a8b"
      },
      "id": "2v7LsWqaq7dj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cO3-TGGQUC1t",
      "metadata": {
        "id": "cO3-TGGQUC1t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OGKiszzdcoN-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "OGKiszzdcoN-",
        "outputId": "d014a74e-54d6-4d76-f7fb-7af9e40949f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of         label                                              tweet\n",
              "0       anger  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...\n",
              "1       anger  Sesama cewe lho (kayaknya), harusnya bisa lebi...\n",
              "2       happy  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...\n",
              "3       anger  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...\n",
              "4       happy  Sharing pengalaman aja, kemarin jam 18.00 bata...\n",
              "...       ...                                                ...\n",
              "4396     love  Tahukah kamu, bahwa saat itu papa memejamkan m...\n",
              "4397     fear  Sulitnya menetapkan Calon Wapresnya Jokowi di ...\n",
              "4398    anger  5. masa depannya nggak jelas. lha iya, gimana ...\n",
              "4399    happy  [USERNAME] dulu beneran ada mahasiswa Teknik U...\n",
              "4400  sadness  Ya Allah, hanya Engkau yang mengetahui rasa sa...\n",
              "\n",
              "[4401 rows x 2 columns]>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.generic.NDFrame.head</b><br/>def head(n: int=5) -&gt; Self</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py</a>Return the first `n` rows.\n",
              "\n",
              "This function returns the first `n` rows for the object based\n",
              "on position. It is useful for quickly testing if your object\n",
              "has the right type of data in it.\n",
              "\n",
              "For negative values of `n`, this function returns all rows except\n",
              "the last `|n|` rows, equivalent to ``df[:n]``.\n",
              "\n",
              "If n is larger than the number of rows, this function returns all rows.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "n : int, default 5\n",
              "    Number of rows to select.\n",
              "\n",
              "Returns\n",
              "-------\n",
              "same type as caller\n",
              "    The first `n` rows of the caller object.\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.tail: Returns the last `n` rows.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; df = pd.DataFrame({&#x27;animal&#x27;: [&#x27;alligator&#x27;, &#x27;bee&#x27;, &#x27;falcon&#x27;, &#x27;lion&#x27;,\n",
              "...                    &#x27;monkey&#x27;, &#x27;parrot&#x27;, &#x27;shark&#x27;, &#x27;whale&#x27;, &#x27;zebra&#x27;]})\n",
              "&gt;&gt;&gt; df\n",
              "      animal\n",
              "0  alligator\n",
              "1        bee\n",
              "2     falcon\n",
              "3       lion\n",
              "4     monkey\n",
              "5     parrot\n",
              "6      shark\n",
              "7      whale\n",
              "8      zebra\n",
              "\n",
              "Viewing the first 5 lines\n",
              "\n",
              "&gt;&gt;&gt; df.head()\n",
              "      animal\n",
              "0  alligator\n",
              "1        bee\n",
              "2     falcon\n",
              "3       lion\n",
              "4     monkey\n",
              "\n",
              "Viewing the first `n` lines (three in this case)\n",
              "\n",
              "&gt;&gt;&gt; df.head(3)\n",
              "      animal\n",
              "0  alligator\n",
              "1        bee\n",
              "2     falcon\n",
              "\n",
              "For negative values of `n`\n",
              "\n",
              "&gt;&gt;&gt; df.head(-3)\n",
              "      animal\n",
              "0  alligator\n",
              "1        bee\n",
              "2     falcon\n",
              "3       lion\n",
              "4     monkey\n",
              "5     parrot</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 5818);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/Twitter_Emotion_Dataset.csv\")\n",
        "df.head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NfCgE1v5MxiC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "NfCgE1v5MxiC",
        "outputId": "b0e62e91-1925-4fb1-835a-ef27bdfcd1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Distribusi Kelas Emosi ---\n",
            "label\n",
            "anger      1101\n",
            "happy      1017\n",
            "sadness     997\n",
            "fear        649\n",
            "love        637\n",
            "Name: count, dtype: int64\n",
            "Grafik distribusi emosi disimpan sebagai 'emotion_distribution.png'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Tampilkan distribusi kelas untuk melihat imbalance\n",
        "print(\"\\n--- Distribusi Kelas Emosi ---\")\n",
        "print(df['label'].value_counts())\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='label', data=df, order=df['label'].value_counts().index)\n",
        "plt.title('Distribusi Emosi dalam Dataset')\n",
        "plt.ylabel('Jumlah Tweet')\n",
        "plt.xlabel('Emosi')\n",
        "plt.tight_layout()\n",
        "plt.savefig('emotion_distribution.png')\n",
        "print(\"Grafik distribusi emosi disimpan sebagai 'emotion_distribution.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YNulm26mc4Uq",
      "metadata": {
        "id": "YNulm26mc4Uq"
      },
      "outputs": [],
      "source": [
        "# 1. PERSIAPAN KAMUS DAN STOPWORD UNTUK PREPROCESSING\n",
        "\n",
        "# Muat kamus alay dan ubah menjadi dictionary\n",
        "alay_df = pd.read_csv(\"/content/kamus_alay.csv\")\n",
        "alay_dict = dict(zip(alay_df[\"slang\"], alay_df[\"formal\"]))\n",
        "# Daftar stopword yang Anda berikan\n",
        "stopwords_list_string = \"\"\"\n",
        "ada adalah adanya adapun agak agaknya agar akan akankah akhir akhiri akhirnya aku akulah amat amatlah anda andalah antar antara antaranya apa apaan apabila apakah apalagi apatah artinya asal asalkan atas atau ataukah ataupun awal awalnya bagai bagaikan bagaimana bagaimanakah bagaimanapun bagi bagian bahkan bahwa bahwasanya baik bakal bakalan balik banyak bapak baru bawah beberapa begini beginian beginikah beginilah begitu begitukah begitulah begitupun bekerja belakang belakangan belum belumlah benar benarkah benarlah berada berakhir berakhirlah berakhirnya berapa berapakah berapalah berapapun berarti berawal berbagai berdatangan beri berikan berikut berikutnya berjumlah berkali-kali berkata berkehendak berkeinginan berkenaan berlainan berlalu berlangsung berlebihan bermacam bermacam-macam bermaksud bermula bersama bersama-sama bersiap bersiap-siap bertanya bertanya-tanya berturut berturut-turut bertutur berujar berupa besar betul betulkah biasa biasanya bila bilakah bisa bisakah boleh bolehkah bolelah buat bukan bukankah bukanlah bukannya bulan bung cara caranya cukup cukupkah cukuplah cuma dahulu dalam dan dapat dari daripada datang dekat demi demikian demikianlah dengan depan di dia diakhiri diakhirinya dialah diantara diantaranya diberi diberikan diberikannya dibuat dibuatnya didapat didatangkan digunakan diibaratkan diibaratkannya diingat diingatkan diinginkan dijawab dijelaskan dijelaskannya dikarenakan dikatakan dikatakannya dikerjakan diketahui diketahuinya dikira dilakukan dilalui dilihat dimaksud dimaksudkan dimaksudkannya dimaksudnya diminta dimintai dimisalkan dimulai dimulailah dimulainya dimungkinkan dini dipastikan diperbuat diperbuatnya dipergunakan diperkirakan diperlihatkan diperlukan diperlukannya dipersoalkan dipertanyakan dipunyai diri dirinya disampaikan disebut disebutkan disebutkannya disini disinilah ditambahkan ditandaskan ditanya ditanyai ditanyakan ditegaskan ditujukan ditunjuk ditunjuki ditunjukkan ditunjukkannya ditunjuknya dituturkan dituturkannya diucapkan diucapkannya diungkapkan dong dua dulu empat enggak enggaknya entah entahlah guna gunakan hal hampir hanya hanyalah hari harus haruslah harusnya hendak hendaklah hendaknya hingga ia ialah ibarat ibaratkan ibaratnya ibu ikut ingat ingat-ingat ingin inginkah inginkan ini inikah inilah itu itukah itulah jadi jadilah jadinya jangan jangankan janganlah jauh jawab jawaban jawabnya jelas jelaskan jelaslah jelasnya jika jikalau juga jumlah jumlahnya justru kala kalau kalaulah kalaupun kalian kami kamilah kamu kamulah kan kapan kapankah kapanpun karena karenanya kasus kata katakan katakanlah katanya ke keadaan kebetulan kecil kedua keduanya keinginan kelamaan kelihatan kelihatannya kelima keluar kembali kemudian kemungkinan kemungkinannya kenapa kepada kepadanya kesampaian keseluruhan keseluruhannya keterlaluan ketika khususnya kini kinilah kira kira-kira kiranya kita kitalah kok kurang lagi lagian lah lain lainnya lalu lama lamanya lanjut lanjutnya lebih lewat lima luar macam maka makanya makin malah malahan mampu mampukah mana manakala manalagi masa masalah masalahnya masih masihkah masing masing-masing mau maupun melainkan melakukan melalui melihat melihatnya memang memastikan memberi memberikan membuat memerlukan memihak meminta memintakan memisalkan memperbuat mempergunakan memperkirakan memperlihatkan mempersiapkan mempersoalkan mempertanyakan mempunyai memulai memungkinkan menaiki menambahkan menandaskan menanti menanti-nanti menantikan menanya menanyai menanyakan mendapat mendapatkan mendatang mendatangi mendatangkan menegaskan mengakhiri mengapa mengatakan mengatakannya mengenai mengerjakan mengetahui menggunakan menghendaki mengibaratkan mengibaratkannya mengingat mengingatkan menginginkan mengira mengucapkan mengucapkannya mengungkapkan menjadi menjawab menjelaskan menuju menunjuk menunjuki menunjukkan menunjuknya menurut menuturkan menyampaikan menyangkut menyatakan menyebutkan menyeluruh menyiapkan merasa mereka merekalah merupakan meski meskipun meyakini meyakinkan minta mirip misal misalkan misalnya mula mulai mulailah mulanya mungkin mungkinkah nah naik namun nanti nantinya nyaris nyatanya oleh olehnya pada padahal padanya pak paling panjang pantas para pasti pastilah penting pentingnya per percuma perlu perlukah perlunya pernah persoalan pertama pertama-tama pertanyaan pertanyakan pihak pihaknya pukul pula pun punya rasa rasanya rata rupanya saat saatnya saja sajalah saling sama sama-sama sambil sampai sampai-sampai sampaikan sana sangat sangatlah satu saya sayalah se sebab sebabnya sebagai sebagaimana sebagainya sebagian sebaik sebaik-baiknya sebaiknya sebaliknya sebanyak sebegini sebegitu sebelum sebelumnya sebenarnya seberapa sebesar sebetulnya sebisanya sebuah sebut sebutlah sebutnya secara secukupnya sedang sedangkan sedemikian sedikit sedikitnya seenaknya segala segalanya segera seharusnya sehingga seingat sejak sejauh sejenak sejumlah sekadar sekadarnya sekali sekali-kali sekalian sekaligus sekalipun sekarang sekarang sekecil seketika sekiranya sekitar sekitarnya sekurang-kurangnya sekurangnya sela selain selaku selalu selama selama-lamanya selamanya selanjutnya seluruh seluruhnya semacam semakin semampu semampunya semasa semasih semata semata-mata semaunya sementara semisal semisalnya sempat semua semuanya semula sendiri sendirian sendirinya seolah seolah-olah seorang sepanjang sepantasnya sepantasnyalah seperlunya seperti sepertinya sepihak sering seringnya serta serupa sesaat sesama sesampai sesegera sesekali seseorang sesuatu sesuatunya sesudah sesudahnya setelah setempat setengah seterusnya setiap setiba setibanya setidak-tidaknya setidaknya setinggi seusai sewaktu siap siapa siapakah siapapun sini sinilah soal soalnya suatu sudah sudahkah sudahlah supaya tadi tadinya tahu tahun tak tambah tambahnya tampak tampaknya tandas tandasnya tanpa tanya tanyakan tanyanya tapi tegas tegasnya telah tempat tengah tentang tentu tentulah tentunya tepat terakhir terasa terbanyak terdahulu terdapat terdiri terhadap terhadapnya teringat teringat-ingat terjadi terjadilah terjadinya terkira terlalu terlebih terlihat termasuk ternyata tersampaikan tersebut tersebutlah tertentu tertuju terus terutama tetap tetapi tiap tiba tiba-tiba tidak tidakkah tidaklah tiga tinggi toh tunjuk turut tutur tuturnya ucap ucapnya ujar ujarnya umum umumnya ungkap ungkapnya untuk usah usai waduh wah wahai waktu waktunya walau walaupun wong yaitu yakin yakni yang\n",
        "\"\"\"\n",
        "# Ubah string menjadi set untuk pencarian yang lebih cepat\n",
        "stopwords_set = set(stopwords_list_string.strip().split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kgih8OrQdi2f",
      "metadata": {
        "id": "kgih8OrQdi2f"
      },
      "outputs": [],
      "source": [
        "# 2. FUNGSI PREPROCESSING\n",
        "\n",
        "def preprocess_tweet(tweet: str, alay_dict: dict, stopwords_set: set) -> str:\n",
        "    # 1. Case Folding (mengubah semua huruf menjadi huruf kecil)\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # 2. Cleansing (hapus URL, mention, hashtag, angka, dan tanda baca)\n",
        "    tweet = re.sub(r\"\\[url\\]|\\[username\\]|#\\w+|\\d+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"[^\\w\\s]\", \"\", tweet)\n",
        "\n",
        "    # 3. Tokenization (pemisahan kata)\n",
        "    tokens = tweet.split()\n",
        "\n",
        "    # 4. Normalization and Stopword Removal\n",
        "    processed_tokens = []\n",
        "    for token in tokens:\n",
        "        # Normalisasi kata alay\n",
        "        normalized_token = alay_dict.get(token, token)\n",
        "\n",
        "        # Hapus stopwords\n",
        "        if normalized_token not in stopwords_set:\n",
        "            processed_tokens.append(normalized_token)\n",
        "\n",
        "    # Gabungkan kembali token menjadi kalimat\n",
        "    return \" \".join(processed_tokens)\n",
        "\n",
        "df['cleaned_tweet'] = df['tweet'].apply(lambda x: preprocess_tweet(x, alay_dict, stopwords_set))\n",
        "\n",
        "# Encode Label\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vaYspe1dhhp4",
      "metadata": {
        "id": "vaYspe1dhhp4"
      },
      "outputs": [],
      "source": [
        "# 3. FEATURE EXTRACTION DENGAN TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, max_features=10000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_tweet'])\n",
        "\n",
        "# 4. PEMBAGIAN DATA\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan: Dilakukan dua ekstraksi fitur, yaitu TF-IDF dan Word2Vec, untuk membuat perbandingan antara pemodelan yang menggunakan ekstraksi fitur yang tidak mempertimbangkan konteks dan makna (TF-IDF), serta pemodelan yang menggunakan ekstrakssi fitur yang memahami makna dan konteks (Word2vec)."
      ],
      "metadata": {
        "id": "KdUCIKnykRul"
      },
      "id": "KdUCIKnykRul"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OT8TLALyOPCE",
      "metadata": {
        "id": "OT8TLALyOPCE"
      },
      "outputs": [],
      "source": [
        "# 5. PEMBAGIAN DATA & PENERAPAN SMOTE\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)\n",
        "smote_tfidf = SMOTE(random_state=42)\n",
        "X_train_resampled_tfidf, y_train_resampled_tfidf = smote_tfidf.fit_resample(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HWsP4uQ4Em05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWsP4uQ4Em05",
        "outputId": "863436f7-89cd-4cae-ebd1-a717a9c2ace3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MEMULAI TUNING UNTUK SVM (DATA SMOTE)\n",
            "==================================================\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "\n",
            "--- Hasil Terbaik SVM (Tuning + SMOTE) ---\n",
            "Parameter Terbaik: {'C': 10, 'kernel': 'rbf'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.61      0.77      0.68       220\n",
            "        fear       0.85      0.59      0.70       130\n",
            "       happy       0.63      0.59      0.61       204\n",
            "        love       0.88      0.60      0.71       127\n",
            "     sadness       0.46      0.54      0.49       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.68      0.62      0.64       881\n",
            "weighted avg       0.65      0.62      0.63       881\n",
            "\n",
            "\n",
            "==================================================\n",
            "MEMULAI TUNING UNTUK LOGISTIC REGRESSION (DATA SMOTE)\n",
            "==================================================\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "\n",
            "--- Hasil Terbaik Logistic Regression (Tuning + SMOTE) ---\n",
            "Parameter Terbaik: {'C': 10, 'solver': 'liblinear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.64      0.68      0.66       220\n",
            "        fear       0.77      0.68      0.72       130\n",
            "       happy       0.61      0.58      0.60       204\n",
            "        love       0.81      0.69      0.74       127\n",
            "     sadness       0.46      0.54      0.49       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.66      0.63      0.64       881\n",
            "weighted avg       0.64      0.62      0.63       881\n",
            "\n",
            "\n",
            "==================================================\n",
            "MEMULAI TUNING UNTUK RANDOM FOREST (DATA SMOTE)\n",
            "==================================================\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "\n",
            "--- Hasil Terbaik Random Forest (Tuning + SMOTE) ---\n",
            "Parameter Terbaik: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.53      0.69      0.60       220\n",
            "        fear       0.83      0.68      0.75       130\n",
            "       happy       0.63      0.48      0.54       204\n",
            "        love       0.82      0.77      0.80       127\n",
            "     sadness       0.45      0.49      0.47       200\n",
            "\n",
            "    accuracy                           0.60       881\n",
            "   macro avg       0.65      0.62      0.63       881\n",
            "weighted avg       0.62      0.60      0.61       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 6. PEMODELAN DAN TUNING\n",
        "\n",
        "# 6a. SVM\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEMULAI TUNING UNTUK SVM (DATA SMOTE)\")\n",
        "print(\"=\"*50)\n",
        "param_grid_svm = {'C': [1, 10], 'kernel': ['linear', 'rbf']}\n",
        "grid_search_svm = GridSearchCV(SVC(random_state=42), param_grid_svm, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_svm.fit(X_train_resampled_tfidf, y_train_resampled_tfidf)\n",
        "best_svm = grid_search_svm.best_estimator_\n",
        "y_pred_svm = best_svm.predict(X_test_tfidf)\n",
        "print(\"\\n--- Hasil Terbaik SVM (Tuning + SMOTE) ---\")\n",
        "print(f\"Parameter Terbaik: {grid_search_svm.best_params_}\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_))\n",
        "\n",
        "# 6b. Logistic Regression\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEMULAI TUNING UNTUK LOGISTIC REGRESSION (DATA SMOTE)\")\n",
        "print(\"=\"*50)\n",
        "param_grid_lr = {'C': [1, 10, 100], 'solver': ['liblinear', 'saga']}\n",
        "grid_search_lr = GridSearchCV(LogisticRegression(random_state=42, max_iter=1000), param_grid_lr, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_lr.fit(X_train_resampled_tfidf, y_train_resampled_tfidf)\n",
        "best_lr = grid_search_lr.best_estimator_\n",
        "y_pred_lr = best_lr.predict(X_test_tfidf)\n",
        "print(\"\\n--- Hasil Terbaik Logistic Regression (Tuning + SMOTE) ---\")\n",
        "print(f\"Parameter Terbaik: {grid_search_lr.best_params_}\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=label_encoder.classes_))\n",
        "\n",
        "# 6c. Random Forest\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEMULAI TUNING UNTUK RANDOM FOREST (DATA SMOTE)\")\n",
        "print(\"=\"*50)\n",
        "param_grid_rf = {'n_estimators': [100, 200], 'max_depth': [50, None], 'min_samples_split': [2, 5]}\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_rf.fit(X_train_resampled_tfidf, y_train_resampled_tfidf)\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_test_tfidf)\n",
        "print(\"\\n--- Hasil Terbaik Random Forest (Tuning + SMOTE) ---\")\n",
        "print(f\"Parameter Terbaik: {grid_search_rf.best_params_}\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan: Dilakukan tuning grid search pada ketiga model agar dapat langsung mendapatkan parameter terbaik.\n",
        "\n",
        "SVM mendapat akurasi 62% dan f1-skor average 0.64, ini menunjukkan jika model ini cukup balance dalam mengklasifikasikan tiap kelas. Bahkan untuk kelas minoritas seperti love dan fear mendapat precision sekitar 0.8 yang berarti model sering benar menebak kelas tersebut. Namun, recall untuk love dan fear masih rendah, yaitu sekitar 0.5-0.6, yang berarti model masih banyak melewatkan data dengan kelas tersebut. Selain itu, kelas sadness memiliki skor paling rendah di antara kelas lain, yang berarti model masih sulit mendeteksi kelas ini.\n",
        "\n",
        "Logistic regression juga mendapat akurasi dan f1-score yang sama dengan SVM, yaitu 62% dan 0.64, ini menunjukkan bahwa model logistic regression ini juga seimbang dalam mengklasifikasikan kelas. Secara keseluruhan, precision dan recall di tiap kelas cukup seimbang dibandingkan dengan SVM.\n",
        "\n",
        "Random forest mendapat akurasi 60% dan f1-score average 0.63, model ini mendapat akurasi paling rendah dibandingkan dengan kedua model lainnya. Namun, random forest mendapat f1-score yang paling tinggi untuk kelas love dan fear, menunjukkan bahwa model ini paling baik dalam mengklasifikasikan kelas minoritas. Namun, dalam mengklasifikasikan kelas mayoritas, model ini tidak begitu baik.\n",
        "\n"
      ],
      "metadata": {
        "id": "opE3PjiBl2VB"
      },
      "id": "opE3PjiBl2VB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kesimpulan Classification"
      ],
      "metadata": {
        "id": "HZa97ocLwJYi"
      },
      "id": "HZa97ocLwJYi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model klasifikasi biasa yang paling baik untuk dataset ini adalah logistic regression karena mendapat akurasi paling tinggi, yaitu 62%. Meskipun SVM juga mendapat akurasi yang sama, skor classification report logistic regression lebih seimbang di semua kelas daripada SVM.\n",
        "\n",
        "Meski begitu, hasil pemodelan secara keseluruhan masih belum begitu baik dan optimal, sehingga perlu dilakukan percobaan model lain menggunakan deep learning."
      ],
      "metadata": {
        "id": "pY31rb3gwSWb"
      },
      "id": "pY31rb3gwSWb"
    },
    {
      "cell_type": "markdown",
      "id": "a3dHNTgibVXc",
      "metadata": {
        "id": "a3dHNTgibVXc"
      },
      "source": [
        "# Deep Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XLNET"
      ],
      "metadata": {
        "id": "v3YPBOMovRys"
      },
      "id": "v3YPBOMovRys"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# 1. Import library\n",
        "# ==============================================\n",
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification, Trainer, TrainingArguments\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# 2. Load dataset\n",
        "# ==============================================\n",
        "df = pd.read_csv(\"/content/Twitter_Emotion_Dataset.csv\")\n",
        "\n",
        "# ==============================================\n",
        "# 3. Preprocessing tweet\n",
        "# ==============================================\n",
        "def clean_tweet(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)      # hapus URL\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)                # hapus mention\n",
        "    text = re.sub(r\"#\", \"\", text)                   # hapus #\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", text)     # hapus simbol\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(clean_tweet)\n",
        "\n",
        "# ==============================================\n",
        "# 4. Encode label\n",
        "# ==============================================\n",
        "le = LabelEncoder()\n",
        "df[\"label_encoded\"] = le.fit_transform(df[\"label\"])\n",
        "num_labels = len(le.classes_)\n",
        "print(\"Kelas:\", list(le.classes_))\n",
        "\n",
        "# ==============================================\n",
        "# 5. Split data train-test\n",
        "# ==============================================\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"tweet\"].tolist(),\n",
        "    df[\"label_encoded\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label_encoded\"]\n",
        ")\n",
        "\n",
        "# ==============================================\n",
        "# 6. Tokenisasi XLNet\n",
        "# ==============================================\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n",
        "\n",
        "# ==============================================\n",
        "# 7. Dataset class\n",
        "# ==============================================\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = TwitterDataset(train_encodings, train_labels)\n",
        "val_dataset = TwitterDataset(val_encodings, val_labels)\n",
        "\n",
        "# ==============================================\n",
        "# 8. Load model XLNet\n",
        "# ==============================================\n",
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=num_labels)\n",
        "\n",
        "# ==============================================\n",
        "# 9. Training arguments\n",
        "# ==============================================\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     evaluation_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     num_train_epochs=2,   # bisa ditambah\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_steps=10,\n",
        "# )\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=1e-5,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# 10. Trainer setup\n",
        "# ==============================================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# ==============================================\n",
        "# 11. Fine-tuning\n",
        "# ==============================================\n",
        "trainer.train()\n",
        "\n",
        "# ==============================================\n",
        "# 12. Evaluasi\n",
        "# ==============================================\n",
        "preds = trainer.predict(val_dataset)\n",
        "y_true = val_labels\n",
        "y_pred = preds.predictions.argmax(-1)\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "NnM8QyCcvU5A",
        "outputId": "f48ec883-ed03-48e0-e139-aa6784d3542d"
      },
      "id": "NnM8QyCcvU5A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kelas: ['anger', 'fear', 'happy', 'love', 'sadness']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='880' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [880/880 03:24, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.631600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.603100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.626600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.587300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.555600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.564400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.575500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.549700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.518800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.483900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.467800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.493500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.422600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.422900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.412400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.42      0.55      0.47       220\n",
            "        fear       0.47      0.07      0.12       130\n",
            "       happy       0.38      0.09      0.14       204\n",
            "        love       0.41      0.65      0.50       127\n",
            "     sadness       0.28      0.46      0.35       200\n",
            "\n",
            "    accuracy                           0.36       881\n",
            "   macro avg       0.39      0.36      0.32       881\n",
            "weighted avg       0.38      0.36      0.32       881\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan: Model XLNet ini mencapai akurasi yang rendah, yaitu 44% dan f1-score yang juga rendah, 0.46.\n",
        "\n",
        "Pada kelas fear, model hampir tidak dapat mendeteksi kelas ini dilihat dari recall-nya yang hanya 0.07, meskipun precision-nya lebih tinggi, ini juga tidak dapat dikatakan baik.\n",
        "\n",
        "Pada kelas happy, recall hanya 0.09 yang berarti model hanya berhasil menemukan 9% kelas happy dari keseluruhan data.\n",
        "\n",
        "Pada kelas anger dan love, recall cukup tinggi, tapi itu berarti model bias dalam mengklasifikasikan terhadap 2 kelas ini, karena recall pada kelas lain sangat jauh lebih rendah.\n",
        "\n",
        "Pada kelas sadness, f1-score 0.35 menunjukkan kinerja yang kurang baik."
      ],
      "metadata": {
        "id": "l1Xov4S6xV70"
      },
      "id": "l1Xov4S6xV70"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1D CNN"
      ],
      "metadata": {
        "id": "cpCIfj_LwQTh"
      },
      "id": "cpCIfj_LwQTh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r9BChx-YbA4t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r9BChx-YbA4t",
        "outputId": "af32b432-068b-435f-edac-ec1853bd91d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset dan kamus alay berhasil dimuat.\n",
            "\n",
            "Memulai preprocessing yang disesuaikan untuk Deep Learning...\n",
            "Preprocessing selesai.\n",
            "\n",
            "Class Weights yang akan digunakan untuk Imbalance Handling:\n",
            "{0: 0.7994550408719346, 1: 1.3562403697996919, 2: 0.8654867256637168, 3: 1.3817896389324962, 4: 0.8828485456369107}\n",
            "\n",
            "Arsitektur Model 1D CNN:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " global_max_pooling1d_2           ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                                                   \n",
              "\n",
              " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)          ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " global_max_pooling1d_2           ?                                   \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                                                   \n",
              "\n",
              " dense_4 (\u001b[38;5;33mDense\u001b[0m)                  ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)              ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_5 (\u001b[38;5;33mDense\u001b[0m)                  ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Memulai pelatihan model...\n",
            "Epoch 1/15\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.2183 - loss: 1.6036 - val_accuracy: 0.4132 - val_loss: 1.5068\n",
            "Epoch 2/15\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.5066 - loss: 1.3067 - val_accuracy: 0.5539 - val_loss: 1.1881\n",
            "Epoch 3/15\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.7131 - loss: 0.8722 - val_accuracy: 0.6186 - val_loss: 0.9822\n",
            "Epoch 4/15\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8918 - loss: 0.4302 - val_accuracy: 0.6390 - val_loss: 0.9996\n",
            "Epoch 5/15\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.9698 - loss: 0.1445 - val_accuracy: 0.6447 - val_loss: 1.1012\n",
            "Epoch 6/15\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.9937 - loss: 0.0567 - val_accuracy: 0.6515 - val_loss: 1.1810\n",
            "\n",
            "Pelatihan selesai.\n",
            "\n",
            "--- Hasil Evaluasi 1D CNN (dengan Penyesuaian) ---\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
            "Akurasi: 0.6515\n",
            "\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.75      0.71       220\n",
            "        fear       0.66      0.71      0.68       130\n",
            "       happy       0.61      0.64      0.62       204\n",
            "        love       0.80      0.78      0.79       127\n",
            "     sadness       0.56      0.43      0.49       200\n",
            "\n",
            "    accuracy                           0.65       881\n",
            "   macro avg       0.66      0.66      0.66       881\n",
            "weighted avg       0.65      0.65      0.65       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# 1. SETUP DAN PREPROCESSING\n",
        "# Muat data\n",
        "try:\n",
        "    df = pd.read_csv('Twitter_Emotion_Dataset.csv')\n",
        "    alay_df = pd.read_csv('kamus_alay.csv')\n",
        "    alay_dict = dict(zip(alay_df['slang'], alay_df['formal']))\n",
        "    print(\"Dataset dan kamus alay berhasil dimuat.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File CSV tidak ditemukan.\")\n",
        "    exit()\n",
        "\n",
        "# Fungsi preprocessing (tidak menghapus stopwords)\n",
        "def preprocess_for_dl(tweet: str, alay_dict: dict) -> str:\n",
        "    tweet = str(tweet)\n",
        "    # 1. Case folding (mengubah semua huruf menjadi huruf kecil)\n",
        "    tweet = tweet.lower()\n",
        "    # 2. Hapus noise (URL, username, hashtag, angka)\n",
        "    tweet = re.sub(r'\\[url\\]|\\[username\\]|#\\w+|\\d+', '', tweet)\n",
        "    # 3. Hapus tanda baca\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "    # 4. Normalisasi kata alay\n",
        "    tokens = tweet.split()\n",
        "    normalized_tokens = [alay_dict.get(token, token) for token in tokens]\n",
        "\n",
        "    return ' '.join(normalized_tokens)\n",
        "\n",
        "print(\"\\nMemulai preprocessing yang disesuaikan untuk Deep Learning...\")\n",
        "df['cleaned_tweet'] = df['tweet'].apply(lambda x: preprocess_for_dl(x, alay_dict))\n",
        "print(\"Preprocessing selesai.\")\n",
        "\n",
        "\n",
        "# 2. PERSIAPAN DATA\n",
        "# Ekstraksi fitur dengan embedding\n",
        "MAX_WORDS = 10000\n",
        "MAX_LEN = 50\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True)\n",
        "tokenizer.fit_on_texts(df['cleaned_tweet'].values)\n",
        "\n",
        "X_seq = tokenizer.texts_to_sequences(df['cleaned_tweet'].values)\n",
        "X_pad = pad_sequences(X_seq, maxlen=MAX_LEN)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "# 3. IMBALANCE HANDLING DENGAN CLASS WEIGHT\n",
        "# Menghitung bobot untuk setiap kelas agar model memberi perhatian lebih pada kelas minoritas\n",
        "class_weights_array = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y),\n",
        "    y=y\n",
        ")\n",
        "# Mengubahnya menjadi format dictionary yang bisa dibaca Keras\n",
        "class_weights = dict(enumerate(class_weights_array))\n",
        "\n",
        "print(\"\\nClass Weights yang akan digunakan untuk Imbalance Handling:\")\n",
        "print(class_weights)\n",
        "\n",
        "\n",
        "# 4. MEMBANGUN MODEL 1D CNN\n",
        "model_cnn_final = Sequential()\n",
        "model_cnn_final.add(Embedding(input_dim=min(MAX_WORDS, len(word_index) + 1),\n",
        "                                output_dim=EMBEDDING_DIM,\n",
        "                                input_length=MAX_LEN))\n",
        "model_cnn_final.add(Conv1D(128, 5, activation='relu'))\n",
        "model_cnn_final.add(GlobalMaxPooling1D())\n",
        "model_cnn_final.add(Dense(128, activation='relu'))\n",
        "model_cnn_final.add(Dropout(0.5))\n",
        "model_cnn_final.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "model_cnn_final.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"\\nArsitektur Model 1D CNN:\")\n",
        "model_cnn_final.summary()\n",
        "\n",
        "\n",
        "# 5. MEMBAGI DATA, MELATIH, DAN MENGEVALUASI\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"\\nMemulai pelatihan model...\")\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
        "\n",
        "history_final = model_cnn_final.fit(X_train, y_train,\n",
        "                                    epochs=15,\n",
        "                                    batch_size=64,\n",
        "                                    validation_data=(X_test, y_test),\n",
        "                                    callbacks=callbacks,\n",
        "                                    class_weight=class_weights) # Menerapkan imbalance handling di sini\n",
        "\n",
        "print(\"\\nPelatihan selesai.\")\n",
        "\n",
        "# 6. EVALUASI\n",
        "print(\"\\n--- Hasil Evaluasi 1D CNN (dengan Penyesuaian) ---\")\n",
        "y_pred_probs = model_cnn_final.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(f\"Akurasi: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nLaporan Klasifikasi:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan: Model 1D CNN mendapat akurasi dan f1-score average yang lebih tinggi daripada model klasifikasi biasa, yaitu 65% dan 0.66, ini menunjukkan bahwa model deep learning ini mampu menangkap pola data yang lebih kompleks daripada model biasa.\n",
        "\n",
        "Pada kelas love, precision dan recall-nya cukup tinggi, yaitu 0.7-0.8, yang berarti model cukup baik dalam mengklasifikasikan tweet love.\n",
        "\n",
        "Pada kelas anger, fear, dan happy, hasilnya sedikit lebih rendah dari love, namun masih cukup baik dibandingkan kelas sadness.\n",
        "\n",
        "Pada kelas sadness, sama seperti model biasa, precision dan recall sangat rendah, yang berarti model masih belum dapat mengklasifikasikan tweet sedih dengan baik meskipun telah menggunakan deep learning."
      ],
      "metadata": {
        "id": "Io5NP8icypTC"
      },
      "id": "Io5NP8icypTC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IndoBERT"
      ],
      "metadata": {
        "id": "EWRq8rS41Wuh"
      },
      "id": "EWRq8rS41Wuh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kDa13zOBLu6p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "kDa13zOBLu6p",
        "outputId": "49bf2f72-3808-45e7-ac3c-be07ce6422cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset dan kamus alay berhasil dimuat.\n",
            "\n",
            "--- Menerapkan Random Oversampling pada Data Latih ---\n",
            "Ukuran data latih sebelum oversampling: 3520\n",
            "Ukuran data latih setelah oversampling: 4405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Memulai proses fine-tuning IndoBERT dengan Data Hasil Oversampling...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1380/1380 12:48, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.771455</td>\n",
              "      <td>0.725312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.483300</td>\n",
              "      <td>0.800764</td>\n",
              "      <td>0.704881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.288400</td>\n",
              "      <td>0.886727</td>\n",
              "      <td>0.730988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.165500</td>\n",
              "      <td>0.959479</td>\n",
              "      <td>0.732123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.092900</td>\n",
              "      <td>1.027076</td>\n",
              "      <td>0.723042</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning selesai.\n",
            "\n",
            "--- Mengevaluasi model terbaik pada data validasi ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Laporan Klasifikasi Final (dengan Oversampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.75      0.74      0.74       220\n",
            "        fear       0.78      0.74      0.76       130\n",
            "       happy       0.85      0.65      0.74       204\n",
            "        love       0.82      0.84      0.83       127\n",
            "     sadness       0.55      0.70      0.62       200\n",
            "\n",
            "    accuracy                           0.73       881\n",
            "   macro avg       0.75      0.73      0.74       881\n",
            "weighted avg       0.74      0.73      0.73       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils import class_weight\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# 1. MEMUAT DATA DAN PREPROCESSING\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('/content/Twitter_Emotion_Dataset.csv')\n",
        "    alay_df = pd.read_csv('/content/kamus_alay.csv')\n",
        "    alay_dict = dict(zip(alay_df['slang'], alay_df['formal']))\n",
        "    print(\"Dataset dan kamus alay berhasil dimuat.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File CSV tidak ditemukan.\")\n",
        "    exit()\n",
        "\n",
        "def preprocess_for_bert(tweet: str, alay_dict: dict) -> str:\n",
        "    tweet = str(tweet).lower() # Case folding (mengubah emua huruf menjadi huruf kecil)\n",
        "    tweet = re.sub(r'\\[url\\]|\\[username\\]|#\\w+|\\d+', '', tweet) # Cleansing (hapus URL, username, hashtag, angka)\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # Hapus tanda baca\n",
        "    tokens = tweet.split()\n",
        "    normalized_tokens = [alay_dict.get(token, token) for token in tokens] # Normalisasi kata alay/slang\n",
        "    return ' '.join(normalized_tokens)\n",
        "\n",
        "df['cleaned_tweet'] = df['tweet'].apply(lambda x: preprocess_for_bert(x, alay_dict))\n",
        "\n",
        "labels = sorted(df['label'].unique().tolist())\n",
        "label_map = {label: i for i, label in enumerate(labels)}\n",
        "df['label_encoded'] = df['label'].map(label_map)\n",
        "\n",
        "# 2. MEMBAGI DATA DAN TOKENISASI\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['cleaned_tweet'].tolist(),\n",
        "    df['label_encoded'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label_encoded'].tolist()\n",
        ")\n",
        "\n",
        "model_name = 'indobenchmark/indobert-base-p1'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "class EmotionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = EmotionDataset(train_encodings, y_train)\n",
        "val_dataset = EmotionDataset(val_encodings, y_val)\n",
        "\n",
        "# 3. OVERSAMPLING DATA LATIH\n",
        "print(\"\\n--- Menerapkan Random Oversampling pada Data Latih ---\")\n",
        "# Resampler hanya bekerja pada data numerik, jadi kita reshape data teks\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_reshaped = np.array(X_train).reshape(-1, 1)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_reshaped, y_train)\n",
        "\n",
        "# Kembalikan X_train ke bentuk list 1D\n",
        "X_train_resampled = X_train_resampled.flatten().tolist()\n",
        "\n",
        "print(f\"Ukuran data latih sebelum oversampling: {len(X_train)}\")\n",
        "print(f\"Ukuran data latih setelah oversampling: {len(X_train_resampled)}\")\n",
        "\n",
        "\n",
        "# 4. TOKENISASI DAN MEMBUAT DATASET\n",
        "model_name = 'indobenchmark/indobert-base-p1'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenisasi data latih yang sudah di-oversample\n",
        "train_encodings = tokenizer(X_train_resampled, truncation=True, padding=True, max_length=128)\n",
        "# Tokenisasi data validasi yang tidak di-oversample\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "class EmotionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = EmotionDataset(train_encodings, y_train_resampled)\n",
        "val_dataset = EmotionDataset(val_encodings, y_val)\n",
        "\n",
        "\n",
        "# 5. FINE-TUNING DENGAN TRAINER STANDAR\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_oversampled',\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\nMemulai proses fine-tuning IndoBERT dengan Data Hasil Oversampling...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning selesai.\")\n",
        "\n",
        "\n",
        "# 6. EVALUASI\n",
        "print(\"\\n--- Mengevaluasi model terbaik pada data validasi ---\")\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "print(\"\\nLaporan Klasifikasi Final (dengan Oversampling):\")\n",
        "print(classification_report(y_val, preds, target_names=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan: Model IndoBERT mendapat akurasi 73% dan f1-score average 0.74, peningkatan skor ini cukup signifikan dibanding model-model sebelumnya.\n",
        "\n",
        "Pada kelas love, lagi-lagi mendapat skor paling tinggi di antara kelas lain pada precision dan recall, yaitu sekitar 0.8, yang berarti model sangat baik dalam mendeteksi emosi ini.\n",
        "\n",
        "Pada kelas fear, anger, dan happy juga mengalami peningkatan skor meskipun tidak setinggi love, namun cukup seimbang di anatar precision dan recall-nya.\n",
        "\n",
        "Pada kelas sadness, skornya juga ikut meningkat, ini karena dilakukan random oversampling sehingga model dapat belajar lebih banyak mengenai data sadness. Meskipun precision-nya masih 0.55, yang berarti model masih sering salah menebak, setidaknya recall 0.7 menunjukkan model sudah lebih berani mendeteksi sadness."
      ],
      "metadata": {
        "id": "4zs0xf132LIQ"
      },
      "id": "4zs0xf132LIQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IndoBERT (Dengan Tambahan Data Baru)"
      ],
      "metadata": {
        "id": "nuoG-nJf61Pz"
      },
      "id": "nuoG-nJf61Pz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menggunakan data tambahan IndoNLU EmoT Dataset dari Hugging Face (https://huggingface.co/datasets/indonlp/indonlu)"
      ],
      "metadata": {
        "id": "Mq9oIIrF7-_s"
      },
      "id": "Mq9oIIrF7-_s"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch scikit-learn pandas imbalanced-learn datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skiycrU9pbyI",
        "outputId": "1de77e7f-91d6-4d33-c542-8249e74631e2"
      },
      "id": "skiycrU9pbyI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# 1. MEMUAT DAN MENGGABUNGKAN SEMUA DATA\n",
        "try:\n",
        "    # Muat semua dataset\n",
        "    df_lama = pd.read_csv(\"/content/Twitter_Emotion_Dataset.csv\")\n",
        "    df_baru_train = pd.read_csv(\"/content/train_preprocess.csv\")\n",
        "    df_baru_valid = pd.read_csv(\"/content/valid_preprocess.csv\")\n",
        "    print(\"Berhasil memuat 3 file dataset.\")\n",
        "\n",
        "    # Gabungkan ketiga DataFrame menjadi satu\n",
        "    df_gabungan = pd.concat([df_lama, df_baru_train, df_baru_valid], ignore_index=True)\n",
        "    print(f\"\\nUkuran dataset gabungan: {len(df_gabungan)} baris\")\n",
        "    print(\"\\nDistribusi label pada dataset gabungan:\")\n",
        "    print(df_gabungan['label'].value_counts())\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: File tidak ditemukan. Pastikan nama file sudah benar. Detail: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 2. PREPROCESSING\n",
        "try:\n",
        "    alay_df = pd.read_csv('kamus_alay.csv')\n",
        "    alay_dict = dict(zip(alay_df['slang'], alay_df['formal']))\n",
        "    print(\"\\nKamus alay berhasil dimuat.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Peringatan: kamus_alay.csv tidak ditemukan. Normalisasi kata alay tidak akan dilakukan.\")\n",
        "    alay_dict = {}\n",
        "\n",
        "# Fungsi preprocessing\n",
        "def preprocess_for_bert(tweet: str, alay_dict: dict) -> str:\n",
        "    tweet = str(tweet)\n",
        "    tweet = tweet.lower() # Case folding\n",
        "    tweet = re.sub(r'\\[url\\]|\\[username\\]|#\\w+|\\d+', '', tweet) # Cleansing\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # Hapus tanda baca\n",
        "    tokens = tweet.split()\n",
        "    normalized_tokens = [alay_dict.get(token, token) for token in tokens] # Normalisasi kata alay/slang\n",
        "    return ' '.join(normalized_tokens)\n",
        "\n",
        "print(\"Memulai preprocessing pada data gabungan...\")\n",
        "df_gabungan['cleaned_tweet'] = df_gabungan['tweet'].apply(lambda x: preprocess_for_bert(x, alay_dict))\n",
        "\n",
        "# Mengubah label teks menjadi angka\n",
        "labels = sorted(df_gabungan['label'].unique().tolist())\n",
        "label_map = {label: i for i, label in enumerate(labels)}\n",
        "df_gabungan['label_encoded'] = df_gabungan['label'].map(label_map)\n",
        "print(\"Preprocessing selesai.\")\n",
        "\n",
        "# 3. MEMBAGI DATA DAN OVERSAMPLING\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df_gabungan['cleaned_tweet'].tolist(),\n",
        "    df_gabungan['label_encoded'].tolist(),\n",
        "    test_size=0.2, # Tetap menggunakan 20% dari data gabungan sebagai validation set\n",
        "    random_state=42,\n",
        "    stratify=df_gabungan['label_encoded'].tolist()\n",
        ")\n",
        "\n",
        "print(\"\\n--- Menerapkan Random Oversampling pada Data Latih ---\")\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_reshaped = np.array(X_train).reshape(-1, 1)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_reshaped, y_train)\n",
        "X_train_resampled = X_train_resampled.flatten().tolist()\n",
        "print(f\"Ukuran data latih setelah oversampling: {len(X_train_resampled)}\")\n",
        "\n",
        "# 4. TOKENISASI DAN MEMBUAT DATASET\n",
        "model_name = 'indobenchmark/indobert-base-p1'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_encodings = tokenizer(X_train_resampled, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "class EmotionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = EmotionDataset(train_encodings, y_train_resampled)\n",
        "val_dataset = EmotionDataset(val_encodings, y_val)\n",
        "\n",
        "# 5. FINE-TUNING INDOBERT DENGAN SETELAN TERBAIK\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_final',\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=1e-5, # Learning rate rendah\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=100, # Disesuaikan karena data latih lebih besar\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\nMemulai proses fine-tuning IndoBERT pada DATA GABUNGAN...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning selesai.\")\n",
        "\n",
        "# 6. EVALUASI\n",
        "print(\"\\n--- Mengevaluasi model terbaik pada data validasi ---\")\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "print(\"\\nLaporan Klasifikasi Final (pada Data Gabungan):\")\n",
        "print(classification_report(y_val, preds, target_names=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "iCRTBtmMf-6V",
        "outputId": "a1985c4b-0ce7-47ef-864e-ba59eedd3fa0"
      },
      "id": "iCRTBtmMf-6V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Berhasil memuat 3 file dataset.\n",
            "\n",
            "Ukuran dataset gabungan: 8362 baris\n",
            "\n",
            "Distribusi label pada dataset gabungan:\n",
            "label\n",
            "anger      2092\n",
            "happy      1933\n",
            "sadness    1894\n",
            "fear       1233\n",
            "love       1210\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Kamus alay berhasil dimuat.\n",
            "Memulai preprocessing pada data gabungan...\n",
            "Preprocessing selesai.\n",
            "\n",
            "--- Menerapkan Random Oversampling pada Data Latih ---\n",
            "Ukuran data latih setelah oversampling: 8370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Memulai proses fine-tuning IndoBERT pada DATA GABUNGAN...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2620' max='2620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2620/2620 15:36, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.562519</td>\n",
              "      <td>0.800956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.262500</td>\n",
              "      <td>0.348901</td>\n",
              "      <td>0.891213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.084000</td>\n",
              "      <td>0.341972</td>\n",
              "      <td>0.915720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.039500</td>\n",
              "      <td>0.288719</td>\n",
              "      <td>0.936043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.018900</td>\n",
              "      <td>0.294451</td>\n",
              "      <td>0.938434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning selesai.\n",
            "\n",
            "--- Mengevaluasi model terbaik pada data validasi ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Laporan Klasifikasi Final (pada Data Gabungan):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.94      0.95      0.95       418\n",
            "        fear       0.93      0.93      0.93       247\n",
            "       happy       0.95      0.95      0.95       387\n",
            "        love       0.98      0.96      0.97       242\n",
            "     sadness       0.90      0.89      0.89       379\n",
            "\n",
            "    accuracy                           0.94      1673\n",
            "   macro avg       0.94      0.94      0.94      1673\n",
            "weighted avg       0.94      0.94      0.94      1673\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan: Ditambahkan data baru sebanyak sekitar 4000 data, sehingga menghasilkan data gabungan sebanyak 8362 data. Penambahan data ini dilakukan agar model dapat belajar dengan lebih maksimal menggunakan data yang mengandung informasi baru mengenai karakteristik masing-masing emosi.\n",
        "\n",
        "Hasilnya, akurasi model ini mencapai 94%, meningkat sangat signifikan dari model sebelumnya tanpa penambahan data.\n",
        "\n",
        "Semua kelas mendapatkan skor 0.9, kecuali recall sadness yang hanya 0.89, tetapi itu juga peningkatan yang sangat tinggi dibandingkan sebelumnya."
      ],
      "metadata": {
        "id": "8540VCi48zBC"
      },
      "id": "8540VCi48zBC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kesimpulan"
      ],
      "metadata": {
        "id": "nPuCwLlIzETX"
      },
      "id": "nPuCwLlIzETX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model terbaik untuk mengklasifikasikan emosi pada dataset ini adalah IndoBERT dengan menggunakan random oversampling dan dengan menambah data baru, serta tuning model dengan learning rate rendah dan epoch yang diperbanyak. Model ini mencapai akurasi 94% dengan skor keseluruhan yang seimbang pada semua kelas, serta tidak terjadi overfitting.\n",
        "\n",
        "Ketika digunakan model IndoBERT tanpa penambahan data baru, akurasinya mencapai 73%, yang mana juga paling tinggi di antara model lain, yang berarti model ini merupakan model yang kinerjanya paling optimal. Oleh karena itu, diputuskan untuk menambahkan data baru menggunakan IndoBERT, agar model dapat lebih banyak mempelajari pola data yang kompleks, sehingga model dapat lebih memahami karakteristik tiap label."
      ],
      "metadata": {
        "id": "_Yiv4pegzIMo"
      },
      "id": "_Yiv4pegzIMo"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}